{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/brjathu/SKD.git"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:11:47.203342Z",
     "iopub.execute_input": "2022-08-21T14:11:47.203913Z",
     "iopub.status.idle": "2022-08-21T14:11:50.816517Z",
     "shell.execute_reply.started": "2022-08-21T14:11:47.203860Z",
     "shell.execute_reply": "2022-08-21T14:11:50.815184Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "wandb_api = \"xxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "wandb.login(key=wandb_api)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:11:51.308976Z",
     "iopub.execute_input": "2022-08-21T14:11:51.309370Z",
     "iopub.status.idle": "2022-08-21T14:11:52.104554Z",
     "shell.execute_reply.started": "2022-08-21T14:11:51.309335Z",
     "shell.execute_reply": "2022-08-21T14:11:52.103526Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pip install wandb --upgrade"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:11:52.485069Z",
     "iopub.execute_input": "2022-08-21T14:11:52.485448Z",
     "iopub.status.idle": "2022-08-21T14:12:02.816157Z",
     "shell.execute_reply.started": "2022-08-21T14:11:52.485416Z",
     "shell.execute_reply": "2022-08-21T14:12:02.814784Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pwd"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:12:02.819473Z",
     "iopub.execute_input": "2022-08-21T14:12:02.819820Z",
     "iopub.status.idle": "2022-08-21T14:12:02.829176Z",
     "shell.execute_reply.started": "2022-08-21T14:12:02.819787Z",
     "shell.execute_reply": "2022-08-21T14:12:02.827975Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cd ./SKD"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:12:02.830865Z",
     "iopub.execute_input": "2022-08-21T14:12:02.831649Z",
     "iopub.status.idle": "2022-08-21T14:12:02.839816Z",
     "shell.execute_reply.started": "2022-08-21T14:12:02.831603Z",
     "shell.execute_reply": "2022-08-21T14:12:02.838770Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:12:02.842733Z",
     "iopub.execute_input": "2022-08-21T14:12:02.843160Z",
     "iopub.status.idle": "2022-08-21T14:13:00.220135Z",
     "shell.execute_reply.started": "2022-08-21T14:12:02.843124Z",
     "shell.execute_reply": "2022-08-21T14:13:00.218477Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!conda install mkl-service -y"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:13:00.228394Z",
     "iopub.execute_input": "2022-08-21T14:13:00.228790Z",
     "iopub.status.idle": "2022-08-21T14:14:02.589935Z",
     "shell.execute_reply.started": "2022-08-21T14:13:00.228748Z",
     "shell.execute_reply": "2022-08-21T14:14:02.588613Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/SKD/models/__init__.py\n",
    "\n",
    "\n",
    "from .resnet_ssl import VAE_resnet18_ssl\n",
    "\n",
    "\n",
    "model_pool = [\n",
    "    \n",
    "    'VAE_resnet18_ssl'\n",
    "]\n",
    "\n",
    "model_dict = {\n",
    "    'VAE_resnet18_ssl': VAE_resnet18_ssl\n",
    "}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:14:02.593244Z",
     "iopub.execute_input": "2022-08-21T14:14:02.593608Z",
     "iopub.status.idle": "2022-08-21T14:14:02.600657Z",
     "shell.execute_reply.started": "2022-08-21T14:14:02.593574Z",
     "shell.execute_reply": "2022-08-21T14:14:02.599658Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-20T18:13:33.872011Z",
     "iopub.execute_input": "2022-08-20T18:13:33.872394Z",
     "iopub.status.idle": "2022-08-20T18:13:33.885098Z",
     "shell.execute_reply.started": "2022-08-20T18:13:33.872362Z",
     "shell.execute_reply": "2022-08-20T18:13:33.883937Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/SKD/models/resnet_ssl.py\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResizeConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, mode='nearest'):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class BasicBlockEnc(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        planes = in_planes*stride\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlockDec(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        planes = int(in_planes/stride)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        # self.bn1 could have been placed here, but that messes up the order of the layers when printing the class\n",
    "\n",
    "        if stride == 1:\n",
    "            self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.conv1 = ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                ResizeConv2d(in_planes, planes, kernel_size=3, scale_factor=stride),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn2(self.conv2(x)))\n",
    "        out = self.bn1(self.conv1(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18Enc(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3,num_classes=0):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.num_classes=num_classes\n",
    "        self.z_dim = z_dim\n",
    "        self.conv1 = nn.Conv2d(nc, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(BasicBlockEnc, 64, num_Blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlockEnc, 128, num_Blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockEnc, 256, num_Blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlockEnc, 512, num_Blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, 2 * z_dim)\n",
    "        if self.num_classes>0:\n",
    "            self.classifier=nn.Linear(512,num_classes) #basic task\n",
    "            self.rot_classifier = nn.Linear(self.num_classes, 4) #rotation task\n",
    "        \n",
    "\n",
    "    def _make_layer(self, BasicBlockEnc, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers += [BasicBlockEnc(self.in_planes, stride)]\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        feat = x.view(x.size(0), -1)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        \n",
    "        xx=self.classifier(feat)\n",
    "        xy=self.rot_classifier(xx)\n",
    "        #xx= self.classifier(x)\n",
    "        #x_rot=self.rot_classifier(xx)\n",
    "        \n",
    "        #x = self.linear(feat)\n",
    "        \n",
    "        #mu = x[:, :self.z_dim]\n",
    "        #logvar = x[:, self.z_dim:]\n",
    "        return xx, xy #, mu, logvar\n",
    "\n",
    "class ResNet18Dec(nn.Module):\n",
    "\n",
    "    def __init__(self, num_Blocks=[2,2,2,2], z_dim=10, nc=3):\n",
    "        super().__init__()\n",
    "        self.in_planes = 512\n",
    "\n",
    "        self.linear = nn.Linear(z_dim, 512)\n",
    "\n",
    "        self.layer4 = self._make_layer(BasicBlockDec, 256, num_Blocks[3], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlockDec, 128, num_Blocks[2], stride=2)\n",
    "        self.layer2 = self._make_layer(BasicBlockDec, 64, num_Blocks[1], stride=2)\n",
    "        self.layer1 = self._make_layer(BasicBlockDec, 64, num_Blocks[0], stride=1)\n",
    "        self.conv1 = ResizeConv2d(64, nc, kernel_size=3, scale_factor=2)\n",
    "\n",
    "    def _make_layer(self, BasicBlockDec, planes, num_Blocks, stride):\n",
    "        strides = [stride] + [1]*(num_Blocks-1)\n",
    "        layers = []\n",
    "        for stride in reversed(strides):\n",
    "            layers += [BasicBlockDec(self.in_planes, stride)]\n",
    "        self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.linear(z)\n",
    "        x = x.view(z.size(0), 512, 1, 1)\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = x.view(x.size(0), 3, 32, 32)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim,num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNet18Enc(z_dim=z_dim,num_classes=num_classes)\n",
    "        #self.decoder = ResNet18Dec(z_dim=z_dim)\n",
    "\n",
    "    def forward(self, x, rot=False):\n",
    "        #xx, mean, logvar = self.encoder(x)\n",
    "        xx, xy = self.encoder(x)\n",
    "        #z = self.reparameterize(mean, logvar)\n",
    "        #x = self.decoder(z)\n",
    "        if rot: #is selfsup\n",
    "            return xx, xy\n",
    "        else:\n",
    "            return xx\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mean, logvar):\n",
    "        std = torch.exp(logvar / 2) # in log-space, squareroot is divide by two\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return epsilon * std + mean\n",
    "\n",
    "def VAE_resnet18_ssl(num_classes,**kwargs):\n",
    "    \"\"\"Constructs a VAE-RESNET18 model.\n",
    "    \"\"\"\n",
    "    model = VAE(num_classes=num_classes,z_dim=10)\n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:14:02.602462Z",
     "iopub.execute_input": "2022-08-21T14:14:02.603448Z",
     "iopub.status.idle": "2022-08-21T14:14:02.632815Z",
     "shell.execute_reply.started": "2022-08-21T14:14:02.603411Z",
     "shell.execute_reply": "2022-08-21T14:14:02.631727Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/SKD/train_selfsupervison.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import mkl\n",
    "\n",
    "# import tensorboard_logger as tb_logger\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import model_pool\n",
    "from models.util import create_model\n",
    "\n",
    "from dataset.mini_imagenet import ImageNet, MetaImageNet\n",
    "from dataset.tiered_imagenet import TieredImageNet, MetaTieredImageNet\n",
    "from dataset.cifar import CIFAR100, MetaCIFAR100\n",
    "from dataset.transform_cfg import transforms_options, transforms_test_options, transforms_list\n",
    "\n",
    "from util import adjust_learning_rate, accuracy, AverageMeter\n",
    "from eval.meta_eval import meta_test, meta_test_tune\n",
    "from eval.cls_eval import validate\n",
    "\n",
    "from models.resnet import resnet12\n",
    "import numpy as np\n",
    "from util import Logger\n",
    "import wandb\n",
    "from dataloader import get_dataloaders\n",
    "\n",
    "def get_freer_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    return np.argmax(memory_available)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(get_freer_gpu())\n",
    "mkl.set_num_threads(2)\n",
    "\n",
    "\n",
    "def parse_option():\n",
    "\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--eval_freq', type=int, default=10, help='meta-eval frequency')\n",
    "    parser.add_argument('--print_freq', type=int, default=100, help='print frequency')\n",
    "    parser.add_argument('--tb_freq', type=int, default=500, help='tb frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=10, help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,80', help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "    parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n",
    "    parser.add_argument('--simclr', type=bool, default=False, help='use simple contrastive learning representation')\n",
    "    parser.add_argument('--ssl', type=bool, default=True, help='use self supervised learning')\n",
    "    parser.add_argument('--tags', type=str, default=\"gen0, ssl\", help='add tags for the experiment')\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet12', choices=model_pool)\n",
    "    parser.add_argument('--dataset', type=str, default='miniImageNet', choices=['miniImageNet', 'tieredImageNet',\n",
    "                                                                                'CIFAR-FS', 'FC100'])\n",
    "    parser.add_argument('--transform', type=str, default='A', choices=transforms_list)\n",
    "    parser.add_argument('--use_trainval', type=bool, help='use trainval set')\n",
    "\n",
    "    # cosine annealing\n",
    "    parser.add_argument('--cosine', action='store_true', help='using cosine annealing')\n",
    "\n",
    "    # specify folder\n",
    "    parser.add_argument('--model_path', type=str, default='save/', help='path to save model')\n",
    "    parser.add_argument('--tb_path', type=str, default='tb/', help='path to tensorboard')\n",
    "    parser.add_argument('--data_root', type=str, default='/raid/data/IncrementLearn/imagenet/Datasets/MiniImagenet/', help='path to data root')\n",
    "\n",
    "    # meta setting\n",
    "    parser.add_argument('--n_test_runs', type=int, default=600, metavar='N',\n",
    "                        help='Number of test runs')\n",
    "    parser.add_argument('--n_ways', type=int, default=5, metavar='N',\n",
    "                        help='Number of classes for doing each classification run')\n",
    "    parser.add_argument('--n_shots', type=int, default=1, metavar='N',\n",
    "                        help='Number of shots in test')\n",
    "    parser.add_argument('--n_queries', type=int, default=15, metavar='N',\n",
    "                        help='Number of query in test')\n",
    "    parser.add_argument('--n_aug_support_samples', default=5, type=int,\n",
    "                        help='The number of augmented samples for each meta test sample')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=1, metavar='test_batch_size',\n",
    "                        help='Size of test batch)')\n",
    "\n",
    "    parser.add_argument('-t', '--trial', type=str, default='1', help='the experiment id')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #hyper parameters\n",
    "    parser.add_argument('--gamma', type=float, default=2, help='loss cofficient for ssl loss')\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    if opt.dataset == 'CIFAR-FS' or opt.dataset == 'FC100':\n",
    "        opt.transform = 'D'\n",
    "\n",
    "    if opt.use_trainval:\n",
    "        opt.trial = opt.trial + '_trainval'\n",
    "\n",
    "    # set the path according to the environment\n",
    "    if not opt.model_path:\n",
    "        opt.model_path = './models_pretrained'\n",
    "    if not opt.tb_path:\n",
    "        opt.tb_path = './tensorboard'\n",
    "    if not opt.data_root:\n",
    "        opt.data_root = './data/{}'.format(opt.dataset)\n",
    "    else:\n",
    "        opt.data_root = '{}/{}'.format(opt.data_root, opt.dataset)\n",
    "    opt.data_aug = True\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "        \n",
    "    tags = opt.tags.split(',')\n",
    "    opt.tags = list([])\n",
    "    for it in tags:\n",
    "        opt.tags.append(it)\n",
    "\n",
    "    opt.model_name = '{}_{}_lr_{}_decay_{}_trans_{}'.format(opt.model, opt.dataset, opt.learning_rate,\n",
    "                                                            opt.weight_decay, opt.transform)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    if opt.adam:\n",
    "        opt.model_name = '{}_useAdam'.format(opt.model_name)\n",
    "\n",
    "    opt.model_name = '{}_trial_{}'.format(opt.model_name, opt.trial)\n",
    "\n",
    "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.tb_folder):\n",
    "        os.makedirs(opt.tb_folder)\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "\n",
    "    opt.n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    \n",
    "    #extras\n",
    "    opt.fresh_start = True\n",
    "    \n",
    "    \n",
    "    return opt\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    opt = parse_option()\n",
    "    wandb.init(project=opt.model_path.split(\"/\")[-1], tags=opt.tags)\n",
    "    wandb.config.update(opt)\n",
    "    wandb.save('*.py')\n",
    "    wandb.run.save()\n",
    "    \n",
    "        \n",
    "    train_loader, val_loader, meta_testloader, meta_valloader, n_cls = get_dataloaders(opt)\n",
    "\n",
    "    # model\n",
    "    model = create_model(opt.model, n_cls, opt.dataset)\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # optimizer\n",
    "    if opt.adam:\n",
    "        print(\"Adam\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=opt.learning_rate,\n",
    "                                     weight_decay=0.0005)\n",
    "    else:\n",
    "        print(\"SGD\")\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=opt.learning_rate,\n",
    "                              momentum=opt.momentum,\n",
    "                              weight_decay=opt.weight_decay)\n",
    "        \n",
    "        \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if opt.n_gpu > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # set cosine annealing scheduler\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.epochs, eta_min, -1)\n",
    "\n",
    "    # routine: supervised pre-training\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "            if opt.cosine:\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                adjust_learning_rate(epoch, opt, optimizer)\n",
    "            print(\"==> training...\")\n",
    "\n",
    "\n",
    "            time1 = time.time()\n",
    "            train_acc, train_acc5, train_loss = train(epoch, train_loader, model, criterion, optimizer, opt)\n",
    "            time2 = time.time()\n",
    "            print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "\n",
    "\n",
    "            val_acc, val_acc_top5, val_loss = validate(val_loader, model, criterion, opt)\n",
    "\n",
    "\n",
    "            #validate\n",
    "            start = time.time()\n",
    "            meta_val_acc, meta_val_std, meta_val_acc5, meta_val_std5 = meta_test(model, meta_valloader,use_logit=True)\n",
    "            test_time = time.time() - start\n",
    "            print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}, Time: {:.1f}'.format(meta_val_acc, meta_val_std, test_time))\n",
    "\n",
    "            #evaluate\n",
    "            start = time.time()\n",
    "            meta_test_acc, meta_test_std, meta_test_acc5, meta_test_std5 = meta_test(model, meta_testloader,use_logit=True)\n",
    "            test_time = time.time() - start\n",
    "            print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}, Time: {:.1f}'.format(meta_test_acc, meta_test_std, test_time))\n",
    "\n",
    "\n",
    "            # regular saving\n",
    "            if epoch % opt.save_freq == 0 or epoch==opt.epochs:\n",
    "                print('==> Saving...')\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model': model.state_dict(),\n",
    "                }            \n",
    "                save_file = os.path.join(opt.save_folder, 'model_'+str(wandb.run.name)+'.pth')\n",
    "                torch.save(state, save_file)\n",
    "\n",
    "                #wandb saving\n",
    "                torch.save(state, os.path.join(wandb.run.dir, \"model.pth\"))\n",
    "\n",
    "                ## onnx saving\n",
    "                #dummy_input = torch.autograd.Variable(torch.randn(1, 3, 32, 32)).cuda()\n",
    "                #torch.onnx.export(model, dummy_input, os.path.join(wandb.run.dir, \"model.onnx\"))\n",
    "\n",
    "            wandb.log({'epoch': epoch, \n",
    "                       'Train Acc': train_acc,\n",
    "                       'Train Acc top 5': train_acc5,\n",
    "                       'Train Loss': train_loss,\n",
    "                       'Val Acc': val_acc,\n",
    "                       'Val Acc top 5': val_acc_top5,\n",
    "                       'Val Loss': val_loss,\n",
    "                       'Meta Test Acc': meta_test_acc,\n",
    "                       'Meta Test std': meta_test_std,\n",
    "                       'Meta Val Acc': meta_val_acc,\n",
    "                       'Meta Val std': meta_val_std,\n",
    "                       'Meta Test Acc5': meta_test_acc5,\n",
    "                       'Meta Test std5': meta_test_std5,\n",
    "                       'Meta Val Acc5': meta_val_acc5,\n",
    "                       'Meta Val std5': meta_val_std5\n",
    "                      })\n",
    "\n",
    "    #final report \n",
    "    generate_final_report(model, opt, wandb)\n",
    "    \n",
    "    #remove output.txt log file \n",
    "    output_log_file = os.path.join(wandb.run.dir, \"output.log\")\n",
    "    if os.path.isfile(output_log_file):\n",
    "        os.remove(output_log_file)\n",
    "    else:    ## Show an error ##\n",
    "        print(\"Error: %s file not found\" % output_log_file)\n",
    "        \n",
    "        \n",
    "        \n",
    "def train(epoch, train_loader, model, criterion, optimizer, opt):\n",
    "    \"\"\"One epoch training\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "        for idx, (input, target, _) in enumerate(pbar):\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            input = input.float()\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            \n",
    "            batch_size = input.size()[0]\n",
    "            x = input\n",
    "            x_90 = x.transpose(2,3).flip(2)\n",
    "            x_180 = x.flip(2).flip(3)\n",
    "            x_270 = x.flip(2).transpose(2,3)\n",
    "            generated_data = torch.cat((x, x_90, x_180, x_270),0)\n",
    "            train_targets = target.repeat(4)\n",
    "            \n",
    "            rot_labels = torch.zeros(4*batch_size).cuda().long()\n",
    "            for i in range(4*batch_size):\n",
    "                if i < batch_size:\n",
    "                    rot_labels[i] = 0\n",
    "                elif i < 2*batch_size:\n",
    "                    rot_labels[i] = 1\n",
    "                elif i < 3*batch_size:\n",
    "                    rot_labels[i] = 2\n",
    "                else:\n",
    "                    rot_labels[i] = 3\n",
    "\n",
    "            # ===================forward=====================\n",
    "            \n",
    "            #Rotation\n",
    "            train_logit, rot_logits = model(generated_data,rot=True)\n",
    "            rot_labels = F.one_hot(rot_labels.to(torch.int64), 4).float()\n",
    "            loss_ss = torch.sum(F.binary_cross_entropy_with_logits(input = rot_logits, target = rot_labels))\n",
    "            loss_ce = criterion(train_logit, train_targets)\n",
    "            \n",
    "            loss = opt.gamma * loss_ss + loss_ce\n",
    "            \n",
    "            #(_,_,_,_, feat), (train_logit, rot_logits) = model(generated_data, rot=True)\n",
    "            \n",
    "            #train_logit, VAE_logit = model(x,selfsup=True)\n",
    "            #train_logit= model(x)\n",
    "            \n",
    "            \n",
    "            #loss_ss = torch.sum(F.binary_cross_entropy_with_logits(input = VAE_logit, target = x))\n",
    "            \n",
    "            #loss_ss = nn.MSELoss(reduction='mean')(VAE_logit, x) #nn.BCELoss(reduction='sum')(VAE_logit, x)\n",
    "            #loss_ce = criterion(train_logit, target)\n",
    "            \n",
    "            #loss = opt.gamma * loss_ss + loss_ce\n",
    "            #loss = loss_ss + loss_ce # 1:(0.75,0.25), 2:(0.25,0.75)\n",
    "            \n",
    "           # loss = criterion(train_logit, target)\n",
    "            \n",
    "            acc1, acc5 = accuracy(train_logit, train_targets, topk=(1, 5)) #this target need to be modified in case of the softmax experiment, because of the additional data, \n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            # ===================backward=====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # ===================meters=====================\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            \n",
    "            pbar.set_postfix({\"Acc@1\":'{0:.2f}'.format(top1.avg.cpu().numpy()), \n",
    "                              \"Acc@5\":'{0:.2f}'.format(top5.avg.cpu().numpy(),2), \n",
    "                              \"Loss\" :'{0:.2f}'.format(losses.avg,2), \n",
    "                             })\n",
    "\n",
    "    print('Train_Acc@1 {top1.avg:.3f} Train_Acc@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg,top5.avg, losses.avg\n",
    "\n",
    "\n",
    "def generate_final_report(model, opt, wandb):\n",
    "    \n",
    "    \n",
    "    opt.n_shots = 1\n",
    "    train_loader, val_loader, meta_testloader, meta_valloader, _ = get_dataloaders(opt)\n",
    "    \n",
    "    #validate\n",
    "    meta_val_acc, meta_val_std, meta_val_acc5, meta_val_std5 = meta_test(model, meta_valloader, use_logit=True)\n",
    "    \n",
    "   # meta_val_acc_feat, meta_val_std_feat = meta_test(model, meta_valloader, use_logit=False)\n",
    "\n",
    "    #evaluate\n",
    "    meta_test_acc, meta_test_std, meta_test_acc5, meta_test_std5 = meta_test(model, meta_testloader, use_logit=True)\n",
    "    \n",
    "    #meta_test_acc_feat, meta_test_std_feat = meta_test(model, meta_testloader, use_logit=False)\n",
    "        \n",
    "    print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}'.format(meta_val_acc, meta_val_std))\n",
    "    #print('Meta Val Acc (feat): {:.4f}, Meta Val std (feat): {:.4f}'.format(meta_val_acc_feat, meta_val_std_feat))\n",
    "    print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}'.format(meta_test_acc, meta_test_std))\n",
    "    #print('Meta Test Acc (feat): {:.4f}, Meta Test std (feat): {:.4f}'.format(meta_test_acc_feat, meta_test_std_feat))\n",
    "    \n",
    "    \n",
    "    wandb.log({'Final Meta Test Acc @1': meta_test_acc,\n",
    "               'Final Meta Test std @1': meta_test_std,\n",
    "               'Final Meta Test Acc @5': meta_test_acc5,\n",
    "               'Final Meta Test std @5': meta_test_std5,\n",
    "               #'Final Meta Test Acc  (feat) @1': meta_test_acc_feat,\n",
    "               #'Final Meta Test std  (feat) @1': meta_test_std_feat,\n",
    "               'Final Meta Val Acc @1': meta_val_acc,\n",
    "               'Final Meta Val std @1': meta_val_std,\n",
    "               'Final Meta Val Acc @5': meta_val_acc5,\n",
    "               'Final Meta Val std @5': meta_val_std5,\n",
    "               #'Final Meta Val Acc   (feat) @1': meta_val_acc_feat,\n",
    "               #'Final Meta Val std   (feat) @1': meta_val_std_feat\n",
    "              })\n",
    "\n",
    "    \n",
    "   # opt.n_shots = 5\n",
    "   # train_loader, val_loader, meta_testloader, meta_valloader, _ = get_dataloaders(opt)\n",
    "    \n",
    "    #validate\n",
    "   # meta_val_acc, meta_val_std = meta_test(model, meta_valloader, use_logit=True)\n",
    "    \n",
    "    #meta_val_acc_feat, meta_val_std_feat = meta_test(model, meta_valloader, use_logit=False)\n",
    "\n",
    "    #evaluate\n",
    "   # meta_test_acc, meta_test_std = meta_test(model, meta_testloader, use_logit=True)\n",
    "    \n",
    "    #meta_test_acc_feat, meta_test_std_feat = meta_test(model, meta_testloader, use_logit=False)\n",
    "        \n",
    "   # print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}'.format(meta_val_acc, meta_val_std))\n",
    "    #print('Meta Val Acc (feat): {:.4f}, Meta Val std (feat): {:.4f}'.format(meta_val_acc_feat, meta_val_std_feat))\n",
    "   # print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}'.format(meta_test_acc, meta_test_std))\n",
    "    #print('Meta Test Acc (feat): {:.4f}, Meta Test std (feat): {:.4f}'.format(meta_test_acc_feat, meta_test_std_feat))\n",
    "\n",
    "    #wandb.log({'Final Meta Test Acc @5': meta_test_acc,\n",
    "     #          'Final Meta Test std @5': meta_test_std,\n",
    "               #'Final Meta Test Acc  (feat) @5': meta_test_acc_feat,\n",
    "               #'Final Meta Test std  (feat) @5': meta_test_std_feat,\n",
    "       #        'Final Meta Val Acc @5': meta_val_acc,\n",
    "      #         'Final Meta Val std @5': meta_val_std,\n",
    "               #'Final Meta Val Acc   (feat) @5': meta_val_acc_feat,\n",
    "               #'Final Meta Val std   (feat) @5': meta_val_std_feat\n",
    "        #      })\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:22:55.370805Z",
     "iopub.execute_input": "2022-08-21T14:22:55.371291Z",
     "iopub.status.idle": "2022-08-21T14:22:55.403578Z",
     "shell.execute_reply.started": "2022-08-21T14:22:55.371240Z",
     "shell.execute_reply": "2022-08-21T14:22:55.402620Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/SKD/eval/meta_eval.py \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import t\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys, os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from util import accuracy\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, data5, confidence=0.95):\n",
    "    a = 100.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * t._ppf((1+confidence)/2., n-1)\n",
    "    \n",
    "    a5 = 100.0 * np.array(data5)\n",
    "    n5 = len(a5)\n",
    "    m5, se5 = np.mean(a5), scipy.stats.sem(a5)\n",
    "    h5 = se5 * t._ppf((1+confidence)/2., n5-1)\n",
    "    return m, h, m5, h5\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    norm = x.pow(2).sum(1, keepdim=True).pow(1. / 2)\n",
    "    out = x.div(norm)\n",
    "    return out\n",
    "\n",
    "\n",
    "def meta_test(net, testloader, use_logit=False, is_norm=True, classifier='LR'):\n",
    "    net = net.eval()\n",
    "    acc = []\n",
    "    acc5 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "            for idx, data in enumerate(pbar):\n",
    "                support_xs, support_ys, support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5 = data\n",
    "\n",
    "                support_xs = support_xs.cuda()\n",
    "                support_xs5 = support_xs5.cuda()\n",
    "                query_xs = query_xs.cuda()\n",
    "                query_xs5 = query_xs5.cuda()\n",
    "                batch_size, _, height, width, channel = support_xs.size()\n",
    "                batch_size5, _, height5, width5, channel5 = support_xs5.size()\n",
    "                support_xs = support_xs.view(-1, height, width, channel)\n",
    "                support_xs5 = support_xs5.view(-1, height5, width5, channel5)\n",
    "                query_xs = query_xs.view(-1, height, width, channel)\n",
    "                query_xs5 = query_xs5.view(-1, height5, width5, channel5)\n",
    "\n",
    "                \n",
    "                \n",
    "#                 batch_size = support_xs.size()[0]\n",
    "#                 x = support_xs\n",
    "#                 x_90 = x.transpose(2,3).flip(2)\n",
    "#                 x_180 = x.flip(2).flip(3)\n",
    "#                 x_270 = x.flip(2).transpose(2,3)\n",
    "#                 generated_data = torch.cat((x, x_90, x_180, x_270),0)\n",
    "#                 support_ys = support_ys.repeat(1,4)\n",
    "#                 support_xs = generated_data\n",
    "            \n",
    "#                 print(support_xs.size())\n",
    "#                 print(support_ys.size())\n",
    "\n",
    "\n",
    "\n",
    "                if use_logit:\n",
    "                    support_features = net(support_xs).view(support_xs.size(0), -1)\n",
    "                    support_features5 = net(support_xs5).view(support_xs5.size(0), -1)\n",
    "                    query_features = net(query_xs).view(query_xs.size(0), -1)\n",
    "                    query_features5 = net(query_xs5).view(query_xs5.size(0), -1)\n",
    "                else:\n",
    "                    feat_support, _ = net(support_xs, is_feat=True)\n",
    "                    support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                    feat_query, _ = net(query_xs, is_feat=True)\n",
    "                    query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "#                     feat_support, _ = net(support_xs)\n",
    "#                     support_features = feat_support.view(support_xs.size(0), -1)\n",
    "#                     feat_query, _ = net(query_xs)\n",
    "#                     query_features = feat_query.view(query_xs.size(0), -1)\n",
    "\n",
    "\n",
    "                if is_norm:\n",
    "                    support_features = normalize(support_features)\n",
    "                    support_features5 = normalize(support_features5)\n",
    "                    query_features = normalize(query_features)\n",
    "                    query_features5 = normalize(query_features5)\n",
    "\n",
    "                support_features = support_features.detach().cpu().numpy()\n",
    "                support_features5 = support_features5.detach().cpu().numpy()\n",
    "                query_features = query_features.detach().cpu().numpy()\n",
    "                query_features5 = query_features5.detach().cpu().numpy()\n",
    "                \n",
    "                support_ys = support_ys.view(-1).numpy()\n",
    "                support_ys5 = support_ys5.view(-1).numpy()\n",
    "                query_ys = query_ys.view(-1).numpy()\n",
    "                query_ys5 = query_ys5.view(-1).numpy()\n",
    "                \n",
    "                \n",
    "                \n",
    "                if classifier == 'LR':\n",
    "                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                                             multi_class='multinomial')\n",
    "                    clf5 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                                             multi_class='multinomial')                         \n",
    "                    clf.fit(support_features, support_ys)\n",
    "                    clf5.fit(support_features5, support_ys5)\n",
    "                    query_ys_pred = clf.predict(query_features)\n",
    "                    query_ys_pred5 = clf5.predict(query_features5)\n",
    "                elif classifier == 'NN':\n",
    "                    query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "                elif classifier == 'Cosine':\n",
    "                    query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "                else:\n",
    "                    raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "                    \n",
    "#                 bs = query_features.shape[0]//opt.n_aug_support_samples\n",
    "#                 a = np.reshape(query_ys_pred[:bs], (-1,1))\n",
    "#                 c = query_ys[:bs]\n",
    "#                 for i in range(1,opt.n_aug_support_samples):\n",
    "#                     a = np.hstack([a, np.reshape(query_ys_pred[i*bs:(i+1)*bs], (-1,1))])\n",
    "                \n",
    "#                 d = [] \n",
    "#                 for i in range(a.shape[0]):\n",
    "#                     b = Counter(a[i,:])\n",
    "#                     d.append(b.most_common(1)[0][0])\n",
    "                \n",
    "# #                 (values,counts) = np.unique(a,axis=1, return_counts=True)\n",
    "# #                 print(counts)\n",
    "# # ind=np.argmax(counts)\n",
    "# # print values[ind]  # pr\n",
    "\n",
    "\n",
    "# # #                 a = np.argmax\n",
    "# #                 print(a.shape)\n",
    "# #                 print(c.shape)\n",
    "                    \n",
    "                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))\n",
    "                acc5.append(metrics.accuracy_score(query_ys5, query_ys_pred5))\n",
    "                \n",
    "                pbar.set_postfix({\"FSL_Acc\":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})\n",
    "    \n",
    "    return mean_confidence_interval(acc,acc5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def meta_test_tune(net, testloader, use_logit=False, is_norm=True, classifier='LR', lamda=0.2):\n",
    "    net = net.eval()\n",
    "    acc = []\n",
    "    \n",
    "    with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "        for idx, data in enumerate(pbar):\n",
    "            support_xs, support_ys, query_xs, query_ys, support_ts, query_ts = data\n",
    "\n",
    "            support_xs = support_xs.cuda()\n",
    "            support_ys = support_ys.cuda()\n",
    "            query_ys = query_ys.cuda()\n",
    "            query_xs = query_xs.cuda()\n",
    "            batch_size, _, height, width, channel = support_xs.size()\n",
    "            support_xs = support_xs.view(-1, height, width, channel)\n",
    "            support_ys = support_ys.view(-1,1)\n",
    "            query_ys = query_ys.view(-1)\n",
    "            query_xs = query_xs.view(-1, height, width, channel)\n",
    "\n",
    "            if use_logit:\n",
    "                support_features = net(support_xs).view(support_xs.size(0), -1)\n",
    "                query_features = net(query_xs).view(query_xs.size(0), -1)\n",
    "            else:\n",
    "                feat_support, _ = net(support_xs, is_feat=True)\n",
    "                support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                feat_query, _ = net(query_xs, is_feat=True)\n",
    "                query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "            if is_norm:\n",
    "                support_features = normalize(support_features)\n",
    "                query_features = normalize(query_features)\n",
    "               \n",
    "            y_onehot = torch.FloatTensor(support_ys.size()[0], 5).cuda()\n",
    "\n",
    "            # In your for loop\n",
    "            y_onehot.zero_()\n",
    "            y_onehot.scatter_(1, support_ys, 1)\n",
    "\n",
    "    \n",
    "            X = support_features\n",
    "            XTX = torch.matmul(torch.t(X),X)\n",
    "            \n",
    "            B = torch.matmul( (XTX + lamda*torch.eye(640).cuda() ).inverse(), torch.matmul(torch.t(X), y_onehot.float()) )\n",
    "#             print(B.size())\n",
    "            m = nn.Sigmoid()\n",
    "            Y_pred = m(torch.matmul(query_features, B))\n",
    "                \n",
    "                \n",
    "#             print(Y_pred, query_ys)\n",
    "#             model = nn.Sequential(nn.Linear(64, 10),nn.LogSoftmax(dim=1))\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#             model.cuda()\n",
    "#             criterion.cuda()\n",
    "#             model.train()\n",
    "            \n",
    "#             for i in range(5):\n",
    "#                 output = model(support_features)\n",
    "#                 loss = criterion(output, support_ys)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward(retain_graph=True) # auto-grad \n",
    "#                 optimizer.step() # update  weights \n",
    "            \n",
    "#             model.eval()\n",
    "#             query_ys_pred = model(query_features)\n",
    "\n",
    "            acc1, acc5 = accuracy(Y_pred, query_ys, topk=(1, 1))\n",
    "            \n",
    "            \n",
    "#             support_features = support_features.detach().cpu().numpy()\n",
    "#             query_features = query_features.detach().cpu().numpy()\n",
    "\n",
    "#             support_ys = support_ys.view(-1).numpy()\n",
    "#             query_ys = query_ys.view(-1).numpy()\n",
    "\n",
    "#             if classifier == 'LR':\n",
    "#                 clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000,\n",
    "#                                          multi_class='multinomial')\n",
    "#                 clf.fit(support_features, support_ys)\n",
    "#                 query_ys_pred = clf.predict(query_features)\n",
    "#             elif classifier == 'NN':\n",
    "#                 query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "#             elif classifier == 'Cosine':\n",
    "#                 query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "#             else:\n",
    "#                 raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "            acc.append(acc1.item()/100.0)\n",
    "\n",
    "            pbar.set_postfix({\"FSL_Acc\":'{0:.4f}'.format(np.mean(acc))})\n",
    "                \n",
    "                \n",
    "    return mean_confidence_interval(acc)\n",
    "\n",
    "\n",
    "\n",
    "def meta_test_ensamble(net, testloader, use_logit=True, is_norm=True, classifier='LR'):\n",
    "    for n in net:\n",
    "        n = n.eval()\n",
    "    acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "            for idx, data in enumerate(pbar):\n",
    "                support_xs, support_ys, query_xs, query_ys = data\n",
    "\n",
    "                support_xs = support_xs.cuda()\n",
    "                query_xs = query_xs.cuda()\n",
    "                batch_size, _, height, width, channel = support_xs.size()\n",
    "                support_xs = support_xs.view(-1, height, width, channel)\n",
    "                query_xs = query_xs.view(-1, height, width, channel)\n",
    "\n",
    "                if use_logit:\n",
    "                    support_features = net[0](support_xs).view(support_xs.size(0), -1)\n",
    "                    query_features = net[0](query_xs).view(query_xs.size(0), -1)\n",
    "                    for n in net[1:]:\n",
    "                        support_features += n(support_xs).view(support_xs.size(0), -1)\n",
    "                        query_features += n(query_xs).view(query_xs.size(0), -1)\n",
    "                else:\n",
    "                    feat_support, _ = net(support_xs, is_feat=True)\n",
    "                    support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                    feat_query, _ = net(query_xs, is_feat=True)\n",
    "                    query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "                if is_norm:\n",
    "                    support_features = normalize(support_features)\n",
    "                    query_features = normalize(query_features)\n",
    "\n",
    "                support_features = support_features.detach().cpu().numpy()\n",
    "                query_features = query_features.detach().cpu().numpy()\n",
    "\n",
    "                support_ys = support_ys.view(-1).numpy()\n",
    "                query_ys = query_ys.view(-1).numpy()\n",
    "\n",
    "                if classifier == 'LR':\n",
    "                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000,\n",
    "                                             multi_class='multinomial')\n",
    "                    clf.fit(support_features, support_ys)\n",
    "                    query_ys_pred = clf.predict(query_features)\n",
    "                elif classifier == 'NN':\n",
    "                    query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "                elif classifier == 'Cosine':\n",
    "                    query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "                else:\n",
    "                    raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))\n",
    "                \n",
    "                pbar.set_postfix({\"FSL_Acc\":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})\n",
    "                \n",
    "    return mean_confidence_interval(acc)\n",
    "\n",
    "\n",
    "def NN(support, support_ys, query):\n",
    "    \"\"\"nearest classifier\"\"\"\n",
    "    support = np.expand_dims(support.transpose(), 0)\n",
    "    query = np.expand_dims(query, 2)\n",
    "\n",
    "    diff = np.multiply(query - support, query - support)\n",
    "    distance = diff.sum(1)\n",
    "    min_idx = np.argmin(distance, axis=1)\n",
    "    pred = [support_ys[idx] for idx in min_idx]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def Cosine(support, support_ys, query):\n",
    "    \"\"\"Cosine classifier\"\"\"\n",
    "    support_norm = np.linalg.norm(support, axis=1, keepdims=True)\n",
    "    support = support / support_norm\n",
    "    query_norm = np.linalg.norm(query, axis=1, keepdims=True)\n",
    "    query = query / query_norm\n",
    "\n",
    "    cosine_distance = query @ support.transpose()\n",
    "    max_idx = np.argmax(cosine_distance, axis=1)\n",
    "    pred = [support_ys[idx] for idx in max_idx]\n",
    "    return pred"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:22:55.862623Z",
     "iopub.execute_input": "2022-08-21T14:22:55.863486Z",
     "iopub.status.idle": "2022-08-21T14:22:55.879470Z",
     "shell.execute_reply.started": "2022-08-21T14:22:55.863445Z",
     "shell.execute_reply": "2022-08-21T14:22:55.878503Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /kaggle/working/SKD/dataset/cifar.py \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CIFAR100(Dataset):\n",
    "    \"\"\"support FC100 and CIFAR-FS\"\"\"\n",
    "    def __init__(self, args, partition='train', pretrain=True, is_sample=False, k=4096,\n",
    "                 transform=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_root = args.data_root\n",
    "        self.partition = partition\n",
    "        self.data_aug = args.data_aug\n",
    "        self.mean = [0.5071, 0.4867, 0.4408]\n",
    "        self.std = [0.2675, 0.2565, 0.2761]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.pretrain = pretrain\n",
    "        self.simclr = args.simclr\n",
    "        \n",
    "        \n",
    "        if transform is None:\n",
    "            if self.partition == 'train' and self.data_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.pretrain:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        else:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        self.data = {}\n",
    "\n",
    "        with open(os.path.join(self.data_root, self.file_pattern % partition), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            self.imgs = data['data']\n",
    "            labels = data['labels']\n",
    "            # adjust sparse labels to labels from 0 to n.\n",
    "            cur_class = 0\n",
    "            label2label = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label not in label2label:\n",
    "                    label2label[label] = cur_class\n",
    "                    cur_class += 1\n",
    "            new_labels = []\n",
    "            for idx, label in enumerate(labels):\n",
    "                new_labels.append(label2label[label])\n",
    "            self.labels = new_labels\n",
    "        \n",
    "        \n",
    "        # pre-process for contrastive sampling\n",
    "        self.k = k\n",
    "        self.is_sample = is_sample\n",
    "        if self.is_sample:\n",
    "            self.labels = np.asarray(self.labels)\n",
    "            self.labels = self.labels - np.min(self.labels)\n",
    "            num_classes = np.max(self.labels) + 1\n",
    "\n",
    "            self.cls_positive = [[] for _ in range(num_classes)]\n",
    "            for i in range(len(self.imgs)):\n",
    "                self.cls_positive[self.labels[i]].append(i)\n",
    "\n",
    "            self.cls_negative = [[] for _ in range(num_classes)]\n",
    "            for i in range(num_classes):\n",
    "                for j in range(num_classes):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    self.cls_negative[i].extend(self.cls_positive[j])\n",
    "\n",
    "            self.cls_positive = [np.asarray(self.cls_positive[i]) for i in range(num_classes)]\n",
    "            self.cls_negative = [np.asarray(self.cls_negative[i]) for i in range(num_classes)]\n",
    "            self.cls_positive = np.asarray(self.cls_positive)\n",
    "            self.cls_negative = np.asarray(self.cls_negative)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = np.asarray(self.imgs[item]).astype('uint8')\n",
    "        target = self.labels[item] - min(self.labels)\n",
    "        \n",
    "        if(self.simclr):\n",
    "            img1 = self.transform(img)\n",
    "            img2 = self.transform(img)\n",
    "            return (img1, img2), target, item\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        if not self.is_sample:\n",
    "            return img, target, item\n",
    "        else:\n",
    "            pos_idx = item\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, item, sample_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CIFAR100_toy(Dataset):\n",
    "    \"\"\"support FC100 and CIFAR-FS\"\"\"\n",
    "    def __init__(self, args, partition='train', pretrain=True, is_sample=False, k=4096,\n",
    "                 transform=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_root = args.data_root\n",
    "        self.partition = partition\n",
    "        self.data_aug = args.data_aug\n",
    "        self.mean = [0.5071, 0.4867, 0.4408]\n",
    "        self.std = [0.2675, 0.2565, 0.2761]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.pretrain = pretrain\n",
    "        self.simclr = args.simclr\n",
    "        \n",
    "        \n",
    "        if transform is None:\n",
    "            if self.partition == 'train' and self.data_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.pretrain:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        else:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        self.data = {}\n",
    "\n",
    "        with open(os.path.join(self.data_root, self.file_pattern % partition), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            self.imgs = data['data']\n",
    "            labels = data['labels']\n",
    "            # adjust sparse labels to labels from 0 to n.\n",
    "            cur_class = 0\n",
    "            label2label = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label not in label2label:\n",
    "                    label2label[label] = cur_class\n",
    "                    cur_class += 1\n",
    "            new_labels = []\n",
    "            for idx, label in enumerate(labels):\n",
    "                new_labels.append(label2label[label])\n",
    "            self.labels = new_labels\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "        self.imgs = np.array(self.imgs)\n",
    "        print(self.labels.shape)\n",
    "        print(self.imgs.shape)\n",
    "        \n",
    "        loc = np.where(self.labels<5)[0]\n",
    "        self.labels = self.labels[loc]\n",
    "        self.imgs   = self.imgs[loc]\n",
    "        \n",
    "        \n",
    "        self.k = k\n",
    "        self.is_sample = is_sample\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = np.asarray(self.imgs[item]).astype('uint8')\n",
    "        target = self.labels[item] - min(self.labels)\n",
    "        \n",
    "        if(self.simclr):\n",
    "            img1 = self.transform(img)\n",
    "            img2 = self.transform(img)\n",
    "            return (img1, img2), target, item\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        if not self.is_sample:\n",
    "            return img, target, item\n",
    "        else:\n",
    "            pos_idx = item\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, item, sample_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class MetaCIFAR100(CIFAR100):\n",
    "\n",
    "    def __init__(self, args, partition='train', train_transform=None, test_transform=None, fix_seed=True):\n",
    "        super(MetaCIFAR100, self).__init__(args, partition, False)\n",
    "        self.fix_seed = fix_seed\n",
    "        self.n_ways = args.n_ways\n",
    "        self.n_shots = args.n_shots\n",
    "        self.n_queries = args.n_queries\n",
    "        self.classes = list(self.data.keys())\n",
    "        self.n_test_runs = args.n_test_runs\n",
    "        self.n_aug_support_samples = args.n_aug_support_samples\n",
    "        if train_transform is None:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                lambda x: np.asarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = train_transform\n",
    "\n",
    "        if test_transform is None:\n",
    "            self.test_transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.test_transform = test_transform\n",
    "\n",
    "        self.data = {}\n",
    "        for idx in range(self.imgs.shape[0]):\n",
    "            if self.labels[idx] not in self.data:\n",
    "                self.data[self.labels[idx]] = []\n",
    "            self.data[self.labels[idx]].append(self.imgs[idx])\n",
    "        self.classes = list(self.data.keys())\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.fix_seed:\n",
    "            np.random.seed(item)\n",
    "        cls_sampled = np.random.choice(self.classes, self.n_ways, False)\n",
    "        \n",
    "        support_xs = []\n",
    "        support_ys = []\n",
    "        support_ts = []\n",
    "        query_xs = []\n",
    "        query_ys = []\n",
    "        query_ts = []\n",
    "        \n",
    "        support_xs5 = []\n",
    "        support_ys5 = []\n",
    "        support_ts5 = []\n",
    "        query_xs5 = []\n",
    "        query_ys5 = []\n",
    "        query_ts5 = []\n",
    "        \n",
    "        for idx, cls in enumerate(cls_sampled):\n",
    "            imgs = np.asarray(self.data[cls]).astype('uint8')\n",
    "            support_xs_ids_sampled = np.random.choice(range(imgs.shape[0]), self.n_shots, False)\n",
    "            support_xs.append(imgs[support_xs_ids_sampled])\n",
    "            support_ys.append([idx] * self.n_shots)\n",
    "            support_ts.append([cls] * self.n_shots)\n",
    "                 \n",
    "            support_xs_ids_sampled5 = np.random.choice(range(imgs.shape[0]), 5, False)\n",
    "            support_xs5.append(imgs[support_xs_ids_sampled5])\n",
    "            #support_xs5.append(imgs[support_xs_ids_sampled])\n",
    "            support_ys5.append([idx] * 5)\n",
    "            support_ts5.append([cls] * 5)\n",
    "            \n",
    "            query_xs_ids = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled)\n",
    "            query_xs_ids = np.random.choice(query_xs_ids, self.n_queries, False)\n",
    "            query_xs.append(imgs[query_xs_ids])\n",
    "            query_ys.append([idx] * query_xs_ids.shape[0])\n",
    "            query_ts.append([cls] * query_xs_ids.shape[0])\n",
    "            \n",
    "            query_xs_ids5 = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled5)\n",
    "            query_xs_ids5 = np.random.choice(query_xs_ids5, self.n_queries, False)\n",
    "            query_xs5.append(imgs[query_xs_ids5])\n",
    "            query_ys5.append([idx] * query_xs_ids5.shape[0])\n",
    "            query_ts5.append([cls] * query_xs_ids5.shape[0])\n",
    "            \n",
    "        support_xs, support_ys,support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5 = np.array(support_xs), np.array(support_ys),np.array(support_xs5), np.array(support_ys5), np.array(\n",
    "            query_xs), np.array(query_ys), np.array(query_xs5),np.array(query_ys5)\n",
    "        \n",
    "        support_ts,support_ts5, query_ts, query_ts5 = np.array(support_ts),np.array(support_ts5), np.array(query_ts),np.array(query_ts5)\n",
    "        \n",
    "        num_ways, n_queries_per_way, height, width, channel = query_xs.shape\n",
    "        query_xs = query_xs.reshape((num_ways * n_queries_per_way, height, width, channel))\n",
    "        query_ys = query_ys.reshape((num_ways * n_queries_per_way,))\n",
    "        query_ts = query_ts.reshape((num_ways * n_queries_per_way,))\n",
    "        \n",
    "        num_ways5, n_queries_per_way5, height5, width5, channel5 = query_xs5.shape\n",
    "        query_xs5 = query_xs5.reshape((num_ways5 * n_queries_per_way5, height5, width5, channel5))\n",
    "        query_ys5 = query_ys5.reshape((num_ways5 * n_queries_per_way5,))\n",
    "        query_ts5 = query_ts5.reshape((num_ways5 * n_queries_per_way5,))\n",
    "\n",
    "        support_xs = support_xs.reshape((-1, height, width, channel))\n",
    "        support_xs5 = support_xs5.reshape((-1, height5, width5, channel5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.n_aug_support_samples > 1:\n",
    "            support_xs = np.tile(support_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            support_ys = np.tile(support_ys.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            support_ts = np.tile(support_ts.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "            support_xs5 = np.tile(support_xs5, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            support_ys5 = np.tile(support_ys5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            support_ts5 = np.tile(support_ts5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "        \n",
    "        \n",
    "        support_xs = np.split(support_xs, support_xs.shape[0], axis=0)\n",
    "        support_xs5 = np.split(support_xs5, support_xs5.shape[0], axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        query_xs = query_xs.reshape((-1, height, width, channel))\n",
    "        query_xs5 = query_xs5.reshape((-1, height5, width5, channel5))\n",
    "        \n",
    "        if self.n_aug_support_samples > 1:\n",
    "            query_xs = np.tile(query_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            query_ys = np.tile(query_ys.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            query_ts = np.tile(query_ts.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "            query_xs5 = np.tile(query_xs5, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            query_ys5 = np.tile(query_ys5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            query_ts5 = np.tile(query_ts5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "        query_xs = np.split(query_xs, query_xs.shape[0], axis=0)\n",
    "        query_xs5 = np.split(query_xs5, query_xs5.shape[0], axis=0)\n",
    "\n",
    "        support_xs = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs)))\n",
    "        support_xs5 = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs5)))\n",
    "        query_xs = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs)))\n",
    "        query_xs5 = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs5)))\n",
    "\n",
    "        return support_xs, support_ys, support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_test_runs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = lambda x: None\n",
    "    args.n_ways = 5\n",
    "    args.n_shots = 1\n",
    "    args.n_queries = 12\n",
    "    # args.data_root = 'data'\n",
    "    args.data_root = '/home/yonglong/Downloads/FC100'\n",
    "    args.data_aug = True\n",
    "    args.n_test_runs = 5\n",
    "    args.n_aug_support_samples = 1\n",
    "    imagenet = CIFAR100(args, 'train')\n",
    "    print(len(imagenet))\n",
    "    print(imagenet.__getitem__(500)[0].shape)\n",
    "\n",
    "    metaimagenet = MetaCIFAR100(args, 'train')\n",
    "    print(len(metaimagenet))\n",
    "    print(metaimagenet.__getitem__(500)[0].size())\n",
    "    print(metaimagenet.__getitem__(500)[1].shape)\n",
    "    print(metaimagenet.__getitem__(500)[2].size())\n",
    "    print(metaimagenet.__getitem__(500)[3].shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:22:57.495106Z",
     "iopub.execute_input": "2022-08-21T14:22:57.495475Z",
     "iopub.status.idle": "2022-08-21T14:22:57.511089Z",
     "shell.execute_reply.started": "2022-08-21T14:22:57.495443Z",
     "shell.execute_reply": "2022-08-21T14:22:57.510075Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#!wandb off"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:10:31.271360Z",
     "iopub.execute_input": "2022-08-21T14:10:31.272677Z",
     "iopub.status.idle": "2022-08-21T14:10:33.700563Z",
     "shell.execute_reply.started": "2022-08-21T14:10:31.272650Z",
     "shell.execute_reply": "2022-08-21T14:10:33.699378Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python /kaggle/working/SKD/train_selfsupervison.py --model VAE_resnet18_ssl --model_path save/CIFAR-FS --dataset CIFAR-FS --data_root /kaggle/input/cifar-fs --epochs 65 --lr_decay_epochs 60 "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-20T18:14:17.685968Z",
     "iopub.execute_input": "2022-08-20T18:14:17.686359Z",
     "iopub.status.idle": "2022-08-20T18:23:42.173619Z",
     "shell.execute_reply.started": "2022-08-20T18:14:17.686326Z",
     "shell.execute_reply": "2022-08-20T18:23:42.172425Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#!python /kaggle/working/SKD/train_selfsupervison.py --model VAE_resnet18_ssl --model_path save/FC100 --dataset FC100 --data_root /kaggle/input/few-shotcifar-100 --epochs 20 --lr_decay_epochs 8"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-21T14:23:00.981103Z",
     "iopub.execute_input": "2022-08-21T14:23:00.981464Z",
     "iopub.status.idle": "2022-08-21T14:32:12.188448Z",
     "shell.execute_reply.started": "2022-08-21T14:23:00.981433Z",
     "shell.execute_reply": "2022-08-21T14:32:12.187297Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
