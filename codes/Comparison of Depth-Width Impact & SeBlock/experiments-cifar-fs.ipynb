{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:50:30.611667Z",
     "iopub.status.busy": "2022-08-18T20:50:30.611250Z",
     "iopub.status.idle": "2022-08-18T20:50:32.441291Z",
     "shell.execute_reply": "2022-08-18T20:50:32.440033Z",
     "shell.execute_reply.started": "2022-08-18T20:50:30.611631Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/brjathu/SKD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:50:32.580152Z",
     "iopub.status.busy": "2022-08-18T20:50:32.579700Z",
     "iopub.status.idle": "2022-08-18T20:50:33.944449Z",
     "shell.execute_reply": "2022-08-18T20:50:33.943206Z",
     "shell.execute_reply.started": "2022-08-18T20:50:32.580114Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb_api = \"χχχχχχχχχχχχχχχχ\"\n",
    "\n",
    "wandb.login(key=wandb_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:50:33.946980Z",
     "iopub.status.busy": "2022-08-18T20:50:33.946571Z",
     "iopub.status.idle": "2022-08-18T20:50:48.882342Z",
     "shell.execute_reply": "2022-08-18T20:50:48.881124Z",
     "shell.execute_reply.started": "2022-08-18T20:50:33.946939Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:50:48.885755Z",
     "iopub.status.busy": "2022-08-18T20:50:48.885008Z",
     "iopub.status.idle": "2022-08-18T20:50:48.893717Z",
     "shell.execute_reply": "2022-08-18T20:50:48.892313Z",
     "shell.execute_reply.started": "2022-08-18T20:50:48.885713Z"
    }
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:50:48.896404Z",
     "iopub.status.busy": "2022-08-18T20:50:48.895296Z",
     "iopub.status.idle": "2022-08-18T20:50:48.903946Z",
     "shell.execute_reply": "2022-08-18T20:50:48.902701Z",
     "shell.execute_reply.started": "2022-08-18T20:50:48.896367Z"
    }
   },
   "outputs": [],
   "source": [
    "cd ./SKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:50:48.908301Z",
     "iopub.status.busy": "2022-08-18T20:50:48.907891Z",
     "iopub.status.idle": "2022-08-18T20:52:55.510452Z",
     "shell.execute_reply": "2022-08-18T20:52:55.509109Z",
     "shell.execute_reply.started": "2022-08-18T20:50:48.908266Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:52:55.513357Z",
     "iopub.status.busy": "2022-08-18T20:52:55.512903Z",
     "iopub.status.idle": "2022-08-18T20:54:02.972734Z",
     "shell.execute_reply": "2022-08-18T20:54:02.971417Z",
     "shell.execute_reply.started": "2022-08-18T20:52:55.513314Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda install mkl-service -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:54:02.975740Z",
     "iopub.status.busy": "2022-08-18T20:54:02.975414Z",
     "iopub.status.idle": "2022-08-18T20:54:02.984131Z",
     "shell.execute_reply": "2022-08-18T20:54:02.982175Z",
     "shell.execute_reply.started": "2022-08-18T20:54:02.975708Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/SKD/models/__init__.py\n",
    "\n",
    "from .resnet_ssl import resnet12_ssl,resnet50,resnet101,seresnet12,seresnet50,seresnet101\n",
    "\n",
    "\n",
    "model_pool = [\n",
    "    'resnet12_ssl',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'seresnet12',\n",
    "    'seresnet50',\n",
    "    'seresnet101'\n",
    "]\n",
    "\n",
    "model_dict = {\n",
    "    'resnet12_ssl': resnet12_ssl,\n",
    "    'resnet50': resnet50,\n",
    "    'resnet101': resnet101,\n",
    "    'seresnet12': seresnet12,\n",
    "    'seresnet50': seresnet50,\n",
    "    'seresnet101':seresnet101\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:54:02.987315Z",
     "iopub.status.busy": "2022-08-18T20:54:02.986862Z",
     "iopub.status.idle": "2022-08-18T20:54:03.020827Z",
     "shell.execute_reply": "2022-08-18T20:54:03.019647Z",
     "shell.execute_reply.started": "2022-08-18T20:54:02.987268Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/SKD/models/resnet_ssl.py\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, channel // reduction),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(channel // reduction, channel),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class DropBlock(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super(DropBlock, self).__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        #self.gamma = gamma\n",
    "        #self.bernouli = Bernoulli(gamma)\n",
    "\n",
    "    def forward(self, x, gamma):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        if self.training:\n",
    "            batch_size, channels, height, width = x.shape\n",
    "            \n",
    "            bernoulli = Bernoulli(gamma)\n",
    "            mask = bernoulli.sample((batch_size, channels, height - (self.block_size - 1), width - (self.block_size - 1))).cuda()\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "            countM = block_mask.size()[0] * block_mask.size()[1] * block_mask.size()[2] * block_mask.size()[3]\n",
    "            count_ones = block_mask.sum()\n",
    "\n",
    "            return block_mask * x * (countM / count_ones)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        left_padding = int((self.block_size-1) / 2)\n",
    "        right_padding = int(self.block_size / 2)\n",
    "        \n",
    "        batch_size, channels, height, width = mask.shape\n",
    "        #print (\"mask\", mask[0][0])\n",
    "        non_zero_idxs = mask.nonzero()\n",
    "        nr_blocks = non_zero_idxs.shape[0]\n",
    "\n",
    "        offsets = torch.stack(\n",
    "            [\n",
    "                torch.arange(self.block_size).view(-1, 1).expand(self.block_size, self.block_size).reshape(-1), # - left_padding,\n",
    "                torch.arange(self.block_size).repeat(self.block_size), #- left_padding\n",
    "            ]\n",
    "        ).t().cuda()\n",
    "        offsets = torch.cat((torch.zeros(self.block_size**2, 2).cuda().long(), offsets.long()), 1)\n",
    "        \n",
    "        if nr_blocks > 0:\n",
    "            non_zero_idxs = non_zero_idxs.repeat(self.block_size ** 2, 1)\n",
    "            offsets = offsets.repeat(nr_blocks, 1).view(-1, 4)\n",
    "            offsets = offsets.long()\n",
    "\n",
    "            block_idxs = non_zero_idxs + offsets\n",
    "            #block_idxs += left_padding\n",
    "            padded_mask = F.pad(mask, (left_padding, right_padding, left_padding, right_padding))\n",
    "            padded_mask[block_idxs[:, 0], block_idxs[:, 1], block_idxs[:, 2], block_idxs[:, 3]] = 1.\n",
    "        else:\n",
    "            padded_mask = F.pad(mask, (left_padding, right_padding, left_padding, right_padding))\n",
    "            \n",
    "        block_mask = 1 - padded_mask#[:height, :width]\n",
    "        return block_mask\n",
    "    \n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0, drop_block=False,\n",
    "                 block_size=1, use_se=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.maxpool = nn.MaxPool2d(stride)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_batches_tracked = 0\n",
    "        self.drop_block = drop_block\n",
    "        self.block_size = block_size\n",
    "        self.DropBlock = DropBlock(block_size=self.block_size)\n",
    "        self.use_se = use_se\n",
    "        if self.use_se:\n",
    "            self.se = SELayer(planes, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.num_batches_tracked += 1\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            if self.drop_block == True:\n",
    "                feat_size = out.size()[2]\n",
    "                keep_rate = max(1.0 - self.drop_rate / (20*2000) * (self.num_batches_tracked), 1.0 - self.drop_rate)\n",
    "                gamma = (1 - keep_rate) / self.block_size**2 * feat_size**2 / (feat_size - self.block_size + 1)**2\n",
    "                out = self.DropBlock(out, gamma=gamma)\n",
    "            else:\n",
    "                out = F.dropout(out, p=self.drop_rate, training=self.training, inplace=True)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, n_blocks, keep_prob=1.0, avg_pool=False, drop_rate=0.0,\n",
    "                 dropblock_size=5, num_classes=-1, use_se=False):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.inplanes = 3\n",
    "        self.use_se = use_se\n",
    "        self.layer1 = self._make_layer(block, n_blocks[0], 16,\n",
    "                                       stride=2, drop_rate=drop_rate)\n",
    "        self.layer2 = self._make_layer(block, n_blocks[1], 40,\n",
    "                                       stride=2, drop_rate=drop_rate)\n",
    "        self.layer3 = self._make_layer(block, n_blocks[2], 80,\n",
    "                                       stride=2, drop_rate=drop_rate, drop_block=True, block_size=dropblock_size)\n",
    "        self.layer4 = self._make_layer(block, n_blocks[3], 160,\n",
    "                                       stride=2, drop_rate=drop_rate, drop_block=True, block_size=dropblock_size)\n",
    "        if avg_pool:\n",
    "            # self.avgpool = nn.AvgPool2d(5, stride=1)\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.keep_prob = keep_prob\n",
    "        self.keep_avg_pool = avg_pool\n",
    "        self.dropout = nn.Dropout(p=1 - self.keep_prob, inplace=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if self.num_classes > 0:\n",
    "            self.classifier = nn.Linear(160, self.num_classes)\n",
    "            self.rot_classifier = nn.Linear(self.num_classes, 4)\n",
    "#             self.rot_classifier1 = nn.Linear(self.num_classes, 32)\n",
    "#             self.rot_classifier2 = nn.Linear(32, 16)\n",
    "#             self.rot_classifier3 = nn.Linear(16, 4)\n",
    "\n",
    "    def _make_layer(self, block, n_block, planes, stride=1, drop_rate=0.0, drop_block=False, block_size=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        if n_block == 1:\n",
    "            layer = block(self.inplanes, planes, stride, downsample, drop_rate, drop_block, block_size, self.use_se)\n",
    "        else:\n",
    "            layer = block(self.inplanes, planes, stride, downsample, drop_rate, self.use_se)\n",
    "        layers.append(layer)\n",
    "        self.inplanes = planes * block.expansion\n",
    "\n",
    "        for i in range(1, n_block):\n",
    "            if i == n_block - 1:\n",
    "                layer = block(self.inplanes, planes, drop_rate=drop_rate, drop_block=drop_block,\n",
    "                              block_size=block_size, use_se=self.use_se)\n",
    "            else:\n",
    "                layer = block(self.inplanes, planes, drop_rate=drop_rate, use_se=self.use_se)\n",
    "            layers.append(layer)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, is_feat=False, rot=False):\n",
    "        x = self.layer1(x)\n",
    "        f0 = x\n",
    "        x = self.layer2(x)\n",
    "        f1 = x\n",
    "        x = self.layer3(x)\n",
    "        f2 = x\n",
    "        x = self.layer4(x)\n",
    "        f3 = x\n",
    "        if self.keep_avg_pool:\n",
    "            x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        feat = x\n",
    "        \n",
    "        xx = self.classifier(x)\n",
    "        \n",
    "        if(rot):\n",
    "#             xy1 = self.rot_classifier1(xx)\n",
    "#             xy2 = self.rot_classifier2(xy1)\n",
    "            xy = self.rot_classifier(xx)\n",
    "            return [f0, f1, f2, f3, feat], (xx, xy)\n",
    "        \n",
    "        if is_feat:\n",
    "            return [f0, f1, f2, f3, feat], xx\n",
    "        else:\n",
    "            return xx\n",
    "\n",
    "\n",
    "def resnet12_ssl(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-12 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], keep_prob=keep_prob, avg_pool=avg_pool, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 2, 2], keep_prob=keep_prob, avg_pool=avg_pool, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet24(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-24 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], keep_prob=keep_prob, avg_pool=avg_pool, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    indeed, only (3 + 4 + 6 + 3) * 3 + 1 = 49 layers\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], keep_prob=keep_prob, avg_pool=avg_pool, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    indeed, only (3 + 4 + 23 + 3) * 3 + 1 = 100 layers\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 23, 3], keep_prob=keep_prob, avg_pool=avg_pool, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def seresnet12(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-12 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], keep_prob=keep_prob, avg_pool=avg_pool, use_se=True, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def seresnet18(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 2, 2], keep_prob=keep_prob, avg_pool=avg_pool, use_se=True, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def seresnet24(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-24 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], keep_prob=keep_prob, avg_pool=avg_pool, use_se=True, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def seresnet50(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    indeed, only (3 + 4 + 6 + 3) * 3 + 1 = 49 layers\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], keep_prob=keep_prob, avg_pool=avg_pool, use_se=True, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def seresnet101(keep_prob=1.0, avg_pool=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    indeed, only (3 + 4 + 23 + 3) * 3 + 1 = 100 layers\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 23, 3], keep_prob=keep_prob, avg_pool=avg_pool, use_se=True, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "    parser.add_argument('--model', type=str, choices=['resnet12', 'resnet18', 'resnet24', 'resnet50', 'resnet101',\n",
    "                                                      'seresnet12', 'seresnet18', 'seresnet24', 'seresnet50',\n",
    "                                                      'seresnet101'])\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_dict = {\n",
    "        'resnet12': resnet12,\n",
    "        'resnet18': resnet18,\n",
    "        'resnet24': resnet24,\n",
    "        'resnet50': resnet50,\n",
    "        'resnet101': resnet101,\n",
    "        'seresnet12': seresnet12,\n",
    "        'seresnet18': seresnet18,\n",
    "        'seresnet24': seresnet24,\n",
    "        'seresnet50': seresnet50,\n",
    "        'seresnet101': seresnet101,\n",
    "    }\n",
    "\n",
    "    model = model_dict[args.model](avg_pool=True, drop_rate=0.1, dropblock_size=5, num_classes=64)\n",
    "    data = torch.randn(2, 3, 84, 84)\n",
    "    model = model.cuda()\n",
    "    data = data.cuda()\n",
    "    feat, logit = model(data, is_feat=True)\n",
    "    print(feat[-1].shape)\n",
    "    print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:54:03.025634Z",
     "iopub.status.busy": "2022-08-18T20:54:03.024664Z",
     "iopub.status.idle": "2022-08-18T20:54:03.043495Z",
     "shell.execute_reply": "2022-08-18T20:54:03.042106Z",
     "shell.execute_reply.started": "2022-08-18T20:54:03.025591Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/SKD/train_selfsupervison.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import mkl\n",
    "\n",
    "# import tensorboard_logger as tb_logger\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import model_pool\n",
    "from models.util import create_model\n",
    "\n",
    "from dataset.mini_imagenet import ImageNet, MetaImageNet\n",
    "from dataset.tiered_imagenet import TieredImageNet, MetaTieredImageNet\n",
    "from dataset.cifar import CIFAR100, MetaCIFAR100\n",
    "from dataset.transform_cfg import transforms_options, transforms_test_options, transforms_list\n",
    "\n",
    "from util import adjust_learning_rate, accuracy, AverageMeter\n",
    "from eval.meta_eval import meta_test, meta_test_tune\n",
    "from eval.cls_eval import validate\n",
    "\n",
    "from models.resnet import resnet12\n",
    "import numpy as np\n",
    "from util import Logger\n",
    "import wandb\n",
    "from dataloader import get_dataloaders\n",
    "\n",
    "def get_freer_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    return np.argmax(memory_available)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(get_freer_gpu())\n",
    "mkl.set_num_threads(2)\n",
    "\n",
    "\n",
    "def parse_option():\n",
    "\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--eval_freq', type=int, default=10, help='meta-eval frequency')\n",
    "    parser.add_argument('--print_freq', type=int, default=100, help='print frequency')\n",
    "    parser.add_argument('--tb_freq', type=int, default=500, help='tb frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=10, help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,80', help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "    parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n",
    "    parser.add_argument('--simclr', type=bool, default=False, help='use simple contrastive learning representation')\n",
    "    parser.add_argument('--ssl', type=bool, default=True, help='use self supervised learning')\n",
    "    parser.add_argument('--tags', type=str, default=\"gen0, ssl\", help='add tags for the experiment')\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet12', choices=model_pool)\n",
    "    parser.add_argument('--dataset', type=str, default='miniImageNet', choices=['miniImageNet', 'tieredImageNet',\n",
    "                                                                                'CIFAR-FS', 'FC100'])\n",
    "    parser.add_argument('--transform', type=str, default='A', choices=transforms_list)\n",
    "    parser.add_argument('--use_trainval', type=bool, help='use trainval set')\n",
    "\n",
    "    # cosine annealing\n",
    "    parser.add_argument('--cosine', action='store_true', help='using cosine annealing')\n",
    "\n",
    "    # specify folder\n",
    "    parser.add_argument('--model_path', type=str, default='save/', help='path to save model')\n",
    "    parser.add_argument('--tb_path', type=str, default='tb/', help='path to tensorboard')\n",
    "    parser.add_argument('--data_root', type=str, default='/raid/data/IncrementLearn/imagenet/Datasets/MiniImagenet/', help='path to data root')\n",
    "\n",
    "    # meta setting\n",
    "    parser.add_argument('--n_test_runs', type=int, default=600, metavar='N',\n",
    "                        help='Number of test runs')\n",
    "    parser.add_argument('--n_ways', type=int, default=5, metavar='N',\n",
    "                        help='Number of classes for doing each classification run')\n",
    "    parser.add_argument('--n_shots', type=int, default=1, metavar='N',\n",
    "                        help='Number of shots in test')\n",
    "    parser.add_argument('--n_queries', type=int, default=15, metavar='N',\n",
    "                        help='Number of query in test')\n",
    "    parser.add_argument('--n_aug_support_samples', default=5, type=int,\n",
    "                        help='The number of augmented samples for each meta test sample')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=1, metavar='test_batch_size',\n",
    "                        help='Size of test batch)')\n",
    "\n",
    "    parser.add_argument('-t', '--trial', type=str, default='1', help='the experiment id')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #hyper parameters\n",
    "    parser.add_argument('--gamma', type=float, default=2, help='loss cofficient for ssl loss')\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    if opt.dataset == 'CIFAR-FS' or opt.dataset == 'FC100':\n",
    "        opt.transform = 'D'\n",
    "\n",
    "    if opt.use_trainval:\n",
    "        opt.trial = opt.trial + '_trainval'\n",
    "\n",
    "    # set the path according to the environment\n",
    "    if not opt.model_path:\n",
    "        opt.model_path = './models_pretrained'\n",
    "    if not opt.tb_path:\n",
    "        opt.tb_path = './tensorboard'\n",
    "    if not opt.data_root:\n",
    "        opt.data_root = './data/{}'.format(opt.dataset)\n",
    "    else:\n",
    "        opt.data_root = '{}/{}'.format(opt.data_root, opt.dataset)\n",
    "    opt.data_aug = True\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "        \n",
    "    tags = opt.tags.split(',')\n",
    "    opt.tags = list([])\n",
    "    for it in tags:\n",
    "        opt.tags.append(it)\n",
    "\n",
    "    opt.model_name = '{}_{}_lr_{}_decay_{}_trans_{}'.format(opt.model, opt.dataset, opt.learning_rate,\n",
    "                                                            opt.weight_decay, opt.transform)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    if opt.adam:\n",
    "        opt.model_name = '{}_useAdam'.format(opt.model_name)\n",
    "\n",
    "    opt.model_name = '{}_trial_{}'.format(opt.model_name, opt.trial)\n",
    "\n",
    "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.tb_folder):\n",
    "        os.makedirs(opt.tb_folder)\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "\n",
    "    opt.n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    \n",
    "    #extras\n",
    "    opt.fresh_start = True\n",
    "    \n",
    "    \n",
    "    return opt\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    opt = parse_option()\n",
    "    wandb.init(project=opt.model_path.split(\"/\")[-1], tags=opt.tags)\n",
    "    wandb.config.update(opt)\n",
    "    wandb.save('*.py')\n",
    "    wandb.run.save()\n",
    "    \n",
    "        \n",
    "    train_loader, val_loader, meta_testloader, meta_valloader, n_cls = get_dataloaders(opt)\n",
    "\n",
    "    # model\n",
    "    model = create_model(opt.model, n_cls, opt.dataset)\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # optimizer\n",
    "    if opt.adam:\n",
    "        print(\"Adam\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=opt.learning_rate,\n",
    "                                     weight_decay=0.0005)\n",
    "    else:\n",
    "        print(\"SGD\")\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=opt.learning_rate,\n",
    "                              momentum=opt.momentum,\n",
    "                              weight_decay=opt.weight_decay)\n",
    "        \n",
    "        \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if opt.n_gpu > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # set cosine annealing scheduler\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.epochs, eta_min, -1)\n",
    "\n",
    "    # routine: supervised pre-training\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "            if opt.cosine:\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                adjust_learning_rate(epoch, opt, optimizer)\n",
    "            print(\"==> training...\")\n",
    "\n",
    "\n",
    "            time1 = time.time()\n",
    "            train_acc, train_acc5, train_loss = train(epoch, train_loader, model, criterion, optimizer, opt)\n",
    "            time2 = time.time()\n",
    "            print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "\n",
    "\n",
    "            val_acc, val_acc_top5, val_loss = validate(val_loader, model, criterion, opt)\n",
    "\n",
    "\n",
    "            #validate\n",
    "            start = time.time()\n",
    "            meta_val_acc, meta_val_std, meta_val_acc5, meta_val_std5 = meta_test(model, meta_valloader,use_logit=True)\n",
    "            test_time = time.time() - start\n",
    "            print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}, Time: {:.1f}'.format(meta_val_acc, meta_val_std, test_time))\n",
    "\n",
    "            #evaluate\n",
    "            start = time.time()\n",
    "            meta_test_acc, meta_test_std, meta_test_acc5, meta_test_std5 = meta_test(model, meta_testloader,use_logit=True)\n",
    "            test_time = time.time() - start\n",
    "            print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}, Time: {:.1f}'.format(meta_test_acc, meta_test_std, test_time))\n",
    "\n",
    "\n",
    "            # regular saving\n",
    "            if epoch % opt.save_freq == 0 or epoch==opt.epochs:\n",
    "                print('==> Saving...')\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model': model.state_dict(),\n",
    "                }            \n",
    "                save_file = os.path.join(opt.save_folder, 'model_'+str(wandb.run.name)+'.pth')\n",
    "                torch.save(state, save_file)\n",
    "\n",
    "                #wandb saving\n",
    "                torch.save(state, os.path.join(wandb.run.dir, \"model.pth\"))\n",
    "\n",
    "                ## onnx saving\n",
    "                #dummy_input = torch.autograd.Variable(torch.randn(1, 3, 32, 32)).cuda()\n",
    "                #torch.onnx.export(model, dummy_input, os.path.join(wandb.run.dir, \"model.onnx\"))\n",
    "\n",
    "            wandb.log({'epoch': epoch, \n",
    "                       'Train Acc': train_acc,\n",
    "                       'Train Acc top 5': train_acc5,\n",
    "                       'Train Loss': train_loss,\n",
    "                       'Val Acc': val_acc,\n",
    "                       'Val Acc top 5': val_acc_top5,\n",
    "                       'Val Loss': val_loss,\n",
    "                       'Meta Test Acc': meta_test_acc,\n",
    "                       'Meta Test std': meta_test_std,\n",
    "                       'Meta Val Acc': meta_val_acc,\n",
    "                       'Meta Val std': meta_val_std,\n",
    "                       'Meta Test Acc5': meta_test_acc5,\n",
    "                       'Meta Test std5': meta_test_std5,\n",
    "                       'Meta Val Acc5': meta_val_acc5,\n",
    "                       'Meta Val std5': meta_val_std5\n",
    "                      })\n",
    "\n",
    "    #final report \n",
    "    generate_final_report(model, opt, wandb)\n",
    "    \n",
    "    #remove output.txt log file \n",
    "    output_log_file = os.path.join(wandb.run.dir, \"output.log\")\n",
    "    if os.path.isfile(output_log_file):\n",
    "        os.remove(output_log_file)\n",
    "    else:    ## Show an error ##\n",
    "        print(\"Error: %s file not found\" % output_log_file)\n",
    "        \n",
    "        \n",
    "        \n",
    "def train(epoch, train_loader, model, criterion, optimizer, opt):\n",
    "    \"\"\"One epoch training\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "        for idx, (input, target, _) in enumerate(pbar):\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            input = input.float()\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            \n",
    "            batch_size = input.size()[0]\n",
    "            x = input\n",
    "            x_90 = x.transpose(2,3).flip(2)\n",
    "            x_180 = x.flip(2).flip(3)\n",
    "            x_270 = x.flip(2).transpose(2,3)\n",
    "            generated_data = torch.cat((x, x_90, x_180, x_270),0)\n",
    "            train_targets = target.repeat(4)\n",
    "            \n",
    "            rot_labels = torch.zeros(4*batch_size).cuda().long()\n",
    "            for i in range(4*batch_size):\n",
    "                if i < batch_size:\n",
    "                    rot_labels[i] = 0\n",
    "                elif i < 2*batch_size:\n",
    "                    rot_labels[i] = 1\n",
    "                elif i < 3*batch_size:\n",
    "                    rot_labels[i] = 2\n",
    "                else:\n",
    "                    rot_labels[i] = 3\n",
    "\n",
    "            # ===================forward=====================\n",
    "            \n",
    "            (_,_,_,_, feat), (train_logit, rot_logits) = model(generated_data, rot=True)\n",
    "            \n",
    "            rot_labels = F.one_hot(rot_labels.to(torch.int64), 4).float()\n",
    "            loss_ss = torch.sum(F.binary_cross_entropy_with_logits(input = rot_logits, target = rot_labels))\n",
    "            loss_ce = criterion(train_logit, train_targets)\n",
    "            \n",
    "            loss = opt.gamma * loss_ss + loss_ce\n",
    "            \n",
    "            acc1, acc5 = accuracy(train_logit, train_targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            # ===================backward=====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # ===================meters=====================\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            \n",
    "            pbar.set_postfix({\"Acc@1\":'{0:.2f}'.format(top1.avg.cpu().numpy()), \n",
    "                              \"Acc@5\":'{0:.2f}'.format(top5.avg.cpu().numpy(),2), \n",
    "                              \"Loss\" :'{0:.2f}'.format(losses.avg,2), \n",
    "                             })\n",
    "\n",
    "    print('Train_Acc@1 {top1.avg:.3f} Train_Acc@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg, losses.avg\n",
    "\n",
    "\n",
    "\n",
    "def generate_final_report(model, opt, wandb):\n",
    "    \n",
    "    \n",
    "    opt.n_shots = 1\n",
    "    train_loader, val_loader, meta_testloader, meta_valloader, _ = get_dataloaders(opt)\n",
    "    \n",
    "    #validate\n",
    "    meta_val_acc, meta_val_std, meta_val_acc5, meta_val_std5 = meta_test(model, meta_valloader, use_logit=True)\n",
    "    \n",
    "   # meta_val_acc_feat, meta_val_std_feat = meta_test(model, meta_valloader, use_logit=False)\n",
    "\n",
    "    #evaluate\n",
    "    meta_test_acc, meta_test_std, meta_test_acc5, meta_test_std5 = meta_test(model, meta_testloader, use_logit=True)\n",
    "    \n",
    "    #meta_test_acc_feat, meta_test_std_feat = meta_test(model, meta_testloader, use_logit=False)\n",
    "        \n",
    "    print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}'.format(meta_val_acc, meta_val_std))\n",
    "    #print('Meta Val Acc (feat): {:.4f}, Meta Val std (feat): {:.4f}'.format(meta_val_acc_feat, meta_val_std_feat))\n",
    "    print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}'.format(meta_test_acc, meta_test_std))\n",
    "    #print('Meta Test Acc (feat): {:.4f}, Meta Test std (feat): {:.4f}'.format(meta_test_acc_feat, meta_test_std_feat))\n",
    "    \n",
    "    \n",
    "    wandb.log({'Final Meta Test Acc @1': meta_test_acc,\n",
    "               'Final Meta Test std @1': meta_test_std,\n",
    "               'Final Meta Test Acc @5': meta_test_acc5,\n",
    "               'Final Meta Test std @5': meta_test_std5,\n",
    "               #'Final Meta Test Acc  (feat) @1': meta_test_acc_feat,\n",
    "               #'Final Meta Test std  (feat) @1': meta_test_std_feat,\n",
    "               'Final Meta Val Acc @1': meta_val_acc,\n",
    "               'Final Meta Val std @1': meta_val_std,\n",
    "               'Final Meta Val Acc @5': meta_val_acc5,\n",
    "               'Final Meta Val std @5': meta_val_std5,\n",
    "               #'Final Meta Val Acc   (feat) @1': meta_val_acc_feat,\n",
    "               #'Final Meta Val std   (feat) @1': meta_val_std_feat\n",
    "              })\n",
    "\n",
    "    \n",
    "   # opt.n_shots = 5\n",
    "   # train_loader, val_loader, meta_testloader, meta_valloader, _ = get_dataloaders(opt)\n",
    "    \n",
    "    #validate\n",
    "   # meta_val_acc, meta_val_std = meta_test(model, meta_valloader, use_logit=True)\n",
    "    \n",
    "    #meta_val_acc_feat, meta_val_std_feat = meta_test(model, meta_valloader, use_logit=False)\n",
    "\n",
    "    #evaluate\n",
    "   # meta_test_acc, meta_test_std = meta_test(model, meta_testloader, use_logit=True)\n",
    "    \n",
    "    #meta_test_acc_feat, meta_test_std_feat = meta_test(model, meta_testloader, use_logit=False)\n",
    "        \n",
    "   # print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}'.format(meta_val_acc, meta_val_std))\n",
    "    #print('Meta Val Acc (feat): {:.4f}, Meta Val std (feat): {:.4f}'.format(meta_val_acc_feat, meta_val_std_feat))\n",
    "   # print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}'.format(meta_test_acc, meta_test_std))\n",
    "    #print('Meta Test Acc (feat): {:.4f}, Meta Test std (feat): {:.4f}'.format(meta_test_acc_feat, meta_test_std_feat))\n",
    "\n",
    "    #wandb.log({'Final Meta Test Acc @5': meta_test_acc,\n",
    "     #          'Final Meta Test std @5': meta_test_std,\n",
    "               #'Final Meta Test Acc  (feat) @5': meta_test_acc_feat,\n",
    "               #'Final Meta Test std  (feat) @5': meta_test_std_feat,\n",
    "       #        'Final Meta Val Acc @5': meta_val_acc,\n",
    "      #         'Final Meta Val std @5': meta_val_std,\n",
    "               #'Final Meta Val Acc   (feat) @5': meta_val_acc_feat,\n",
    "               #'Final Meta Val std   (feat) @5': meta_val_std_feat\n",
    "        #      })\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:54:03.047135Z",
     "iopub.status.busy": "2022-08-18T20:54:03.045455Z",
     "iopub.status.idle": "2022-08-18T20:54:03.063090Z",
     "shell.execute_reply": "2022-08-18T20:54:03.062022Z",
     "shell.execute_reply.started": "2022-08-18T20:54:03.047101Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/SKD/eval/meta_eval.py \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import t\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys, os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from util import accuracy\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, data5, confidence=0.95):\n",
    "    a = 100.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * t._ppf((1+confidence)/2., n-1)\n",
    "    \n",
    "    a5 = 100.0 * np.array(data5)\n",
    "    n5 = len(a5)\n",
    "    m5, se5 = np.mean(a5), scipy.stats.sem(a5)\n",
    "    h5 = se5 * t._ppf((1+confidence)/2., n5-1)\n",
    "    return m, h, m5, h5\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    norm = x.pow(2).sum(1, keepdim=True).pow(1. / 2)\n",
    "    out = x.div(norm)\n",
    "    return out\n",
    "\n",
    "\n",
    "def meta_test(net, testloader, use_logit=False, is_norm=True, classifier='LR'):\n",
    "    net = net.eval()\n",
    "    acc = []\n",
    "    acc5 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "            for idx, data in enumerate(pbar):\n",
    "                support_xs, support_ys, support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5 = data\n",
    "\n",
    "                support_xs = support_xs.cuda()\n",
    "                support_xs5 = support_xs5.cuda()\n",
    "                query_xs = query_xs.cuda()\n",
    "                query_xs5 = query_xs5.cuda()\n",
    "                batch_size, _, height, width, channel = support_xs.size()\n",
    "                batch_size5, _, height5, width5, channel5 = support_xs5.size()\n",
    "                support_xs = support_xs.view(-1, height, width, channel)\n",
    "                support_xs5 = support_xs5.view(-1, height5, width5, channel5)\n",
    "                query_xs = query_xs.view(-1, height, width, channel)\n",
    "                query_xs5 = query_xs5.view(-1, height5, width5, channel5)\n",
    "\n",
    "                \n",
    "                \n",
    "#                 batch_size = support_xs.size()[0]\n",
    "#                 x = support_xs\n",
    "#                 x_90 = x.transpose(2,3).flip(2)\n",
    "#                 x_180 = x.flip(2).flip(3)\n",
    "#                 x_270 = x.flip(2).transpose(2,3)\n",
    "#                 generated_data = torch.cat((x, x_90, x_180, x_270),0)\n",
    "#                 support_ys = support_ys.repeat(1,4)\n",
    "#                 support_xs = generated_data\n",
    "            \n",
    "#                 print(support_xs.size())\n",
    "#                 print(support_ys.size())\n",
    "\n",
    "\n",
    "\n",
    "                if use_logit:\n",
    "                    support_features = net(support_xs).view(support_xs.size(0), -1)\n",
    "                    support_features5 = net(support_xs5).view(support_xs5.size(0), -1)\n",
    "                    query_features = net(query_xs).view(query_xs.size(0), -1)\n",
    "                    query_features5 = net(query_xs5).view(query_xs5.size(0), -1)\n",
    "                else:\n",
    "                    feat_support, _ = net(support_xs, is_feat=True)\n",
    "                    support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                    feat_query, _ = net(query_xs, is_feat=True)\n",
    "                    query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "#                     feat_support, _ = net(support_xs)\n",
    "#                     support_features = feat_support.view(support_xs.size(0), -1)\n",
    "#                     feat_query, _ = net(query_xs)\n",
    "#                     query_features = feat_query.view(query_xs.size(0), -1)\n",
    "\n",
    "\n",
    "                if is_norm:\n",
    "                    support_features = normalize(support_features)\n",
    "                    support_features5 = normalize(support_features5)\n",
    "                    query_features = normalize(query_features)\n",
    "                    query_features5 = normalize(query_features5)\n",
    "\n",
    "                support_features = support_features.detach().cpu().numpy()\n",
    "                support_features5 = support_features5.detach().cpu().numpy()\n",
    "                query_features = query_features.detach().cpu().numpy()\n",
    "                query_features5 = query_features5.detach().cpu().numpy()\n",
    "                \n",
    "                support_ys = support_ys.view(-1).numpy()\n",
    "                support_ys5 = support_ys5.view(-1).numpy()\n",
    "                query_ys = query_ys.view(-1).numpy()\n",
    "                query_ys5 = query_ys5.view(-1).numpy()\n",
    "                \n",
    "                \n",
    "                \n",
    "                if classifier == 'LR':\n",
    "                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                                             multi_class='multinomial')\n",
    "                    clf5 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                                             multi_class='multinomial')                         \n",
    "                    clf.fit(support_features, support_ys)\n",
    "                    clf5.fit(support_features5, support_ys5)\n",
    "                    query_ys_pred = clf.predict(query_features)\n",
    "                    query_ys_pred5 = clf5.predict(query_features5)\n",
    "                elif classifier == 'NN':\n",
    "                    query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "                elif classifier == 'Cosine':\n",
    "                    query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "                else:\n",
    "                    raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "                    \n",
    "#                 bs = query_features.shape[0]//opt.n_aug_support_samples\n",
    "#                 a = np.reshape(query_ys_pred[:bs], (-1,1))\n",
    "#                 c = query_ys[:bs]\n",
    "#                 for i in range(1,opt.n_aug_support_samples):\n",
    "#                     a = np.hstack([a, np.reshape(query_ys_pred[i*bs:(i+1)*bs], (-1,1))])\n",
    "                \n",
    "#                 d = [] \n",
    "#                 for i in range(a.shape[0]):\n",
    "#                     b = Counter(a[i,:])\n",
    "#                     d.append(b.most_common(1)[0][0])\n",
    "                \n",
    "# #                 (values,counts) = np.unique(a,axis=1, return_counts=True)\n",
    "# #                 print(counts)\n",
    "# # ind=np.argmax(counts)\n",
    "# # print values[ind]  # pr\n",
    "\n",
    "\n",
    "# # #                 a = np.argmax\n",
    "# #                 print(a.shape)\n",
    "# #                 print(c.shape)\n",
    "                    \n",
    "                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))\n",
    "                acc5.append(metrics.accuracy_score(query_ys5, query_ys_pred5))\n",
    "                \n",
    "                pbar.set_postfix({\"FSL_Acc\":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})\n",
    "    \n",
    "    return mean_confidence_interval(acc,acc5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def meta_test_tune(net, testloader, use_logit=False, is_norm=True, classifier='LR', lamda=0.2):\n",
    "    net = net.eval()\n",
    "    acc = []\n",
    "    \n",
    "    with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "        for idx, data in enumerate(pbar):\n",
    "            support_xs, support_ys, query_xs, query_ys, support_ts, query_ts = data\n",
    "\n",
    "            support_xs = support_xs.cuda()\n",
    "            support_ys = support_ys.cuda()\n",
    "            query_ys = query_ys.cuda()\n",
    "            query_xs = query_xs.cuda()\n",
    "            batch_size, _, height, width, channel = support_xs.size()\n",
    "            support_xs = support_xs.view(-1, height, width, channel)\n",
    "            support_ys = support_ys.view(-1,1)\n",
    "            query_ys = query_ys.view(-1)\n",
    "            query_xs = query_xs.view(-1, height, width, channel)\n",
    "\n",
    "            if use_logit:\n",
    "                support_features = net(support_xs).view(support_xs.size(0), -1)\n",
    "                query_features = net(query_xs).view(query_xs.size(0), -1)\n",
    "            else:\n",
    "                feat_support, _ = net(support_xs, is_feat=True)\n",
    "                support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                feat_query, _ = net(query_xs, is_feat=True)\n",
    "                query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "            if is_norm:\n",
    "                support_features = normalize(support_features)\n",
    "                query_features = normalize(query_features)\n",
    "               \n",
    "            y_onehot = torch.FloatTensor(support_ys.size()[0], 5).cuda()\n",
    "\n",
    "            # In your for loop\n",
    "            y_onehot.zero_()\n",
    "            y_onehot.scatter_(1, support_ys, 1)\n",
    "\n",
    "    \n",
    "            X = support_features\n",
    "            XTX = torch.matmul(torch.t(X),X)\n",
    "            \n",
    "            B = torch.matmul( (XTX + lamda*torch.eye(640).cuda() ).inverse(), torch.matmul(torch.t(X), y_onehot.float()) )\n",
    "#             print(B.size())\n",
    "            m = nn.Sigmoid()\n",
    "            Y_pred = m(torch.matmul(query_features, B))\n",
    "                \n",
    "                \n",
    "#             print(Y_pred, query_ys)\n",
    "#             model = nn.Sequential(nn.Linear(64, 10),nn.LogSoftmax(dim=1))\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#             model.cuda()\n",
    "#             criterion.cuda()\n",
    "#             model.train()\n",
    "            \n",
    "#             for i in range(5):\n",
    "#                 output = model(support_features)\n",
    "#                 loss = criterion(output, support_ys)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward(retain_graph=True) # auto-grad \n",
    "#                 optimizer.step() # update  weights \n",
    "            \n",
    "#             model.eval()\n",
    "#             query_ys_pred = model(query_features)\n",
    "\n",
    "            acc1, acc5 = accuracy(Y_pred, query_ys, topk=(1, 1))\n",
    "            \n",
    "            \n",
    "#             support_features = support_features.detach().cpu().numpy()\n",
    "#             query_features = query_features.detach().cpu().numpy()\n",
    "\n",
    "#             support_ys = support_ys.view(-1).numpy()\n",
    "#             query_ys = query_ys.view(-1).numpy()\n",
    "\n",
    "#             if classifier == 'LR':\n",
    "#                 clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000,\n",
    "#                                          multi_class='multinomial')\n",
    "#                 clf.fit(support_features, support_ys)\n",
    "#                 query_ys_pred = clf.predict(query_features)\n",
    "#             elif classifier == 'NN':\n",
    "#                 query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "#             elif classifier == 'Cosine':\n",
    "#                 query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "#             else:\n",
    "#                 raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "            acc.append(acc1.item()/100.0)\n",
    "\n",
    "            pbar.set_postfix({\"FSL_Acc\":'{0:.4f}'.format(np.mean(acc))})\n",
    "                \n",
    "                \n",
    "    return mean_confidence_interval(acc)\n",
    "\n",
    "\n",
    "\n",
    "def meta_test_ensamble(net, testloader, use_logit=True, is_norm=True, classifier='LR'):\n",
    "    for n in net:\n",
    "        n = n.eval()\n",
    "    acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "            for idx, data in enumerate(pbar):\n",
    "                support_xs, support_ys, query_xs, query_ys = data\n",
    "\n",
    "                support_xs = support_xs.cuda()\n",
    "                query_xs = query_xs.cuda()\n",
    "                batch_size, _, height, width, channel = support_xs.size()\n",
    "                support_xs = support_xs.view(-1, height, width, channel)\n",
    "                query_xs = query_xs.view(-1, height, width, channel)\n",
    "\n",
    "                if use_logit:\n",
    "                    support_features = net[0](support_xs).view(support_xs.size(0), -1)\n",
    "                    query_features = net[0](query_xs).view(query_xs.size(0), -1)\n",
    "                    for n in net[1:]:\n",
    "                        support_features += n(support_xs).view(support_xs.size(0), -1)\n",
    "                        query_features += n(query_xs).view(query_xs.size(0), -1)\n",
    "                else:\n",
    "                    feat_support, _ = net(support_xs, is_feat=True)\n",
    "                    support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                    feat_query, _ = net(query_xs, is_feat=True)\n",
    "                    query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "                if is_norm:\n",
    "                    support_features = normalize(support_features)\n",
    "                    query_features = normalize(query_features)\n",
    "\n",
    "                support_features = support_features.detach().cpu().numpy()\n",
    "                query_features = query_features.detach().cpu().numpy()\n",
    "\n",
    "                support_ys = support_ys.view(-1).numpy()\n",
    "                query_ys = query_ys.view(-1).numpy()\n",
    "\n",
    "                if classifier == 'LR':\n",
    "                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000,\n",
    "                                             multi_class='multinomial')\n",
    "                    clf.fit(support_features, support_ys)\n",
    "                    query_ys_pred = clf.predict(query_features)\n",
    "                elif classifier == 'NN':\n",
    "                    query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "                elif classifier == 'Cosine':\n",
    "                    query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "                else:\n",
    "                    raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))\n",
    "                \n",
    "                pbar.set_postfix({\"FSL_Acc\":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})\n",
    "                \n",
    "    return mean_confidence_interval(acc)\n",
    "\n",
    "\n",
    "def NN(support, support_ys, query):\n",
    "    \"\"\"nearest classifier\"\"\"\n",
    "    support = np.expand_dims(support.transpose(), 0)\n",
    "    query = np.expand_dims(query, 2)\n",
    "\n",
    "    diff = np.multiply(query - support, query - support)\n",
    "    distance = diff.sum(1)\n",
    "    min_idx = np.argmin(distance, axis=1)\n",
    "    pred = [support_ys[idx] for idx in min_idx]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def Cosine(support, support_ys, query):\n",
    "    \"\"\"Cosine classifier\"\"\"\n",
    "    support_norm = np.linalg.norm(support, axis=1, keepdims=True)\n",
    "    support = support / support_norm\n",
    "    query_norm = np.linalg.norm(query, axis=1, keepdims=True)\n",
    "    query = query / query_norm\n",
    "\n",
    "    cosine_distance = query @ support.transpose()\n",
    "    max_idx = np.argmax(cosine_distance, axis=1)\n",
    "    pred = [support_ys[idx] for idx in max_idx]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:54:03.068317Z",
     "iopub.status.busy": "2022-08-18T20:54:03.067740Z",
     "iopub.status.idle": "2022-08-18T20:54:03.084327Z",
     "shell.execute_reply": "2022-08-18T20:54:03.083028Z",
     "shell.execute_reply.started": "2022-08-18T20:54:03.068289Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/SKD/dataset/cifar.py \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CIFAR100(Dataset):\n",
    "    \"\"\"support FC100 and CIFAR-FS\"\"\"\n",
    "    def __init__(self, args, partition='train', pretrain=True, is_sample=False, k=4096,\n",
    "                 transform=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_root = args.data_root\n",
    "        self.partition = partition\n",
    "        self.data_aug = args.data_aug\n",
    "        self.mean = [0.5071, 0.4867, 0.4408]\n",
    "        self.std = [0.2675, 0.2565, 0.2761]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.pretrain = pretrain\n",
    "        self.simclr = args.simclr\n",
    "        \n",
    "        \n",
    "        if transform is None:\n",
    "            if self.partition == 'train' and self.data_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.pretrain:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        else:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        self.data = {}\n",
    "\n",
    "        with open(os.path.join(self.data_root, self.file_pattern % partition), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            self.imgs = data['data']\n",
    "            labels = data['labels']\n",
    "            # adjust sparse labels to labels from 0 to n.\n",
    "            cur_class = 0\n",
    "            label2label = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label not in label2label:\n",
    "                    label2label[label] = cur_class\n",
    "                    cur_class += 1\n",
    "            new_labels = []\n",
    "            for idx, label in enumerate(labels):\n",
    "                new_labels.append(label2label[label])\n",
    "            self.labels = new_labels\n",
    "        \n",
    "        \n",
    "        # pre-process for contrastive sampling\n",
    "        self.k = k\n",
    "        self.is_sample = is_sample\n",
    "        if self.is_sample:\n",
    "            self.labels = np.asarray(self.labels)\n",
    "            self.labels = self.labels - np.min(self.labels)\n",
    "            num_classes = np.max(self.labels) + 1\n",
    "\n",
    "            self.cls_positive = [[] for _ in range(num_classes)]\n",
    "            for i in range(len(self.imgs)):\n",
    "                self.cls_positive[self.labels[i]].append(i)\n",
    "\n",
    "            self.cls_negative = [[] for _ in range(num_classes)]\n",
    "            for i in range(num_classes):\n",
    "                for j in range(num_classes):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    self.cls_negative[i].extend(self.cls_positive[j])\n",
    "\n",
    "            self.cls_positive = [np.asarray(self.cls_positive[i]) for i in range(num_classes)]\n",
    "            self.cls_negative = [np.asarray(self.cls_negative[i]) for i in range(num_classes)]\n",
    "            self.cls_positive = np.asarray(self.cls_positive)\n",
    "            self.cls_negative = np.asarray(self.cls_negative)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = np.asarray(self.imgs[item]).astype('uint8')\n",
    "        target = self.labels[item] - min(self.labels)\n",
    "        \n",
    "        if(self.simclr):\n",
    "            img1 = self.transform(img)\n",
    "            img2 = self.transform(img)\n",
    "            return (img1, img2), target, item\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        if not self.is_sample:\n",
    "            return img, target, item\n",
    "        else:\n",
    "            pos_idx = item\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, item, sample_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CIFAR100_toy(Dataset):\n",
    "    \"\"\"support FC100 and CIFAR-FS\"\"\"\n",
    "    def __init__(self, args, partition='train', pretrain=True, is_sample=False, k=4096,\n",
    "                 transform=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_root = args.data_root\n",
    "        self.partition = partition\n",
    "        self.data_aug = args.data_aug\n",
    "        self.mean = [0.5071, 0.4867, 0.4408]\n",
    "        self.std = [0.2675, 0.2565, 0.2761]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.pretrain = pretrain\n",
    "        self.simclr = args.simclr\n",
    "        \n",
    "        \n",
    "        if transform is None:\n",
    "            if self.partition == 'train' and self.data_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.pretrain:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        else:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        self.data = {}\n",
    "\n",
    "        with open(os.path.join(self.data_root, self.file_pattern % partition), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            self.imgs = data['data']\n",
    "            labels = data['labels']\n",
    "            # adjust sparse labels to labels from 0 to n.\n",
    "            cur_class = 0\n",
    "            label2label = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label not in label2label:\n",
    "                    label2label[label] = cur_class\n",
    "                    cur_class += 1\n",
    "            new_labels = []\n",
    "            for idx, label in enumerate(labels):\n",
    "                new_labels.append(label2label[label])\n",
    "            self.labels = new_labels\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "        self.imgs = np.array(self.imgs)\n",
    "        print(self.labels.shape)\n",
    "        print(self.imgs.shape)\n",
    "        \n",
    "        loc = np.where(self.labels<5)[0]\n",
    "        self.labels = self.labels[loc]\n",
    "        self.imgs   = self.imgs[loc]\n",
    "        \n",
    "        \n",
    "        self.k = k\n",
    "        self.is_sample = is_sample\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = np.asarray(self.imgs[item]).astype('uint8')\n",
    "        target = self.labels[item] - min(self.labels)\n",
    "        \n",
    "        if(self.simclr):\n",
    "            img1 = self.transform(img)\n",
    "            img2 = self.transform(img)\n",
    "            return (img1, img2), target, item\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        if not self.is_sample:\n",
    "            return img, target, item\n",
    "        else:\n",
    "            pos_idx = item\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, item, sample_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class MetaCIFAR100(CIFAR100):\n",
    "\n",
    "    def __init__(self, args, partition='train', train_transform=None, test_transform=None, fix_seed=True):\n",
    "        super(MetaCIFAR100, self).__init__(args, partition, False)\n",
    "        self.fix_seed = fix_seed\n",
    "        self.n_ways = args.n_ways\n",
    "        self.n_shots = args.n_shots\n",
    "        self.n_queries = args.n_queries\n",
    "        self.classes = list(self.data.keys())\n",
    "        self.n_test_runs = args.n_test_runs\n",
    "        self.n_aug_support_samples = args.n_aug_support_samples\n",
    "        if train_transform is None:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                lambda x: np.asarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = train_transform\n",
    "\n",
    "        if test_transform is None:\n",
    "            self.test_transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.test_transform = test_transform\n",
    "\n",
    "        self.data = {}\n",
    "        for idx in range(self.imgs.shape[0]):\n",
    "            if self.labels[idx] not in self.data:\n",
    "                self.data[self.labels[idx]] = []\n",
    "            self.data[self.labels[idx]].append(self.imgs[idx])\n",
    "        self.classes = list(self.data.keys())\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.fix_seed:\n",
    "            np.random.seed(item)\n",
    "        cls_sampled = np.random.choice(self.classes, self.n_ways, False)\n",
    "        \n",
    "        support_xs = []\n",
    "        support_ys = []\n",
    "        support_ts = []\n",
    "        query_xs = []\n",
    "        query_ys = []\n",
    "        query_ts = []\n",
    "        \n",
    "        support_xs5 = []\n",
    "        support_ys5 = []\n",
    "        support_ts5 = []\n",
    "        query_xs5 = []\n",
    "        query_ys5 = []\n",
    "        query_ts5 = []\n",
    "        \n",
    "        for idx, cls in enumerate(cls_sampled):\n",
    "            imgs = np.asarray(self.data[cls]).astype('uint8')\n",
    "            support_xs_ids_sampled = np.random.choice(range(imgs.shape[0]), self.n_shots, False)\n",
    "            support_xs.append(imgs[support_xs_ids_sampled])\n",
    "            support_ys.append([idx] * self.n_shots)\n",
    "            support_ts.append([cls] * self.n_shots)\n",
    "                 \n",
    "            support_xs_ids_sampled5 = np.random.choice(range(imgs.shape[0]), 5, False)\n",
    "            support_xs5.append(imgs[support_xs_ids_sampled5])\n",
    "            #support_xs5.append(imgs[support_xs_ids_sampled])\n",
    "            support_ys5.append([idx] * 5)\n",
    "            support_ts5.append([cls] * 5)\n",
    "            \n",
    "            query_xs_ids = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled)\n",
    "            query_xs_ids = np.random.choice(query_xs_ids, self.n_queries, False)\n",
    "            query_xs.append(imgs[query_xs_ids])\n",
    "            query_ys.append([idx] * query_xs_ids.shape[0])\n",
    "            query_ts.append([cls] * query_xs_ids.shape[0])\n",
    "            \n",
    "            query_xs_ids5 = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled5)\n",
    "            query_xs_ids5 = np.random.choice(query_xs_ids5, self.n_queries, False)\n",
    "            query_xs5.append(imgs[query_xs_ids5])\n",
    "            query_ys5.append([idx] * query_xs_ids5.shape[0])\n",
    "            query_ts5.append([cls] * query_xs_ids5.shape[0])\n",
    "            \n",
    "        support_xs, support_ys,support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5 = np.array(support_xs), np.array(support_ys),np.array(support_xs5), np.array(support_ys5), np.array(\n",
    "            query_xs), np.array(query_ys), np.array(query_xs5),np.array(query_ys5)\n",
    "        \n",
    "        support_ts,support_ts5, query_ts, query_ts5 = np.array(support_ts),np.array(support_ts5), np.array(query_ts),np.array(query_ts5)\n",
    "        \n",
    "        num_ways, n_queries_per_way, height, width, channel = query_xs.shape\n",
    "        query_xs = query_xs.reshape((num_ways * n_queries_per_way, height, width, channel))\n",
    "        query_ys = query_ys.reshape((num_ways * n_queries_per_way,))\n",
    "        query_ts = query_ts.reshape((num_ways * n_queries_per_way,))\n",
    "        \n",
    "        num_ways5, n_queries_per_way5, height5, width5, channel5 = query_xs5.shape\n",
    "        query_xs5 = query_xs5.reshape((num_ways5 * n_queries_per_way5, height5, width5, channel5))\n",
    "        query_ys5 = query_ys5.reshape((num_ways5 * n_queries_per_way5,))\n",
    "        query_ts5 = query_ts5.reshape((num_ways5 * n_queries_per_way5,))\n",
    "\n",
    "        support_xs = support_xs.reshape((-1, height, width, channel))\n",
    "        support_xs5 = support_xs5.reshape((-1, height5, width5, channel5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.n_aug_support_samples > 1:\n",
    "            support_xs = np.tile(support_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            support_ys = np.tile(support_ys.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            support_ts = np.tile(support_ts.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "            support_xs5 = np.tile(support_xs5, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            support_ys5 = np.tile(support_ys5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            support_ts5 = np.tile(support_ts5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "        \n",
    "        \n",
    "        support_xs = np.split(support_xs, support_xs.shape[0], axis=0)\n",
    "        support_xs5 = np.split(support_xs5, support_xs5.shape[0], axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        query_xs = query_xs.reshape((-1, height, width, channel))\n",
    "        query_xs5 = query_xs5.reshape((-1, height5, width5, channel5))\n",
    "        \n",
    "        if self.n_aug_support_samples > 1:\n",
    "            query_xs = np.tile(query_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            query_ys = np.tile(query_ys.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            query_ts = np.tile(query_ts.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "            query_xs5 = np.tile(query_xs5, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            query_ys5 = np.tile(query_ys5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            query_ts5 = np.tile(query_ts5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "        query_xs = np.split(query_xs, query_xs.shape[0], axis=0)\n",
    "        query_xs5 = np.split(query_xs5, query_xs5.shape[0], axis=0)\n",
    "\n",
    "        support_xs = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs)))\n",
    "        support_xs5 = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs5)))\n",
    "        query_xs = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs)))\n",
    "        query_xs5 = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs5)))\n",
    "\n",
    "        return support_xs, support_ys, support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_test_runs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = lambda x: None\n",
    "    args.n_ways = 5\n",
    "    args.n_shots = 1\n",
    "    args.n_queries = 12\n",
    "    # args.data_root = 'data'\n",
    "    args.data_root = '/home/yonglong/Downloads/FC100'\n",
    "    args.data_aug = True\n",
    "    args.n_test_runs = 5\n",
    "    args.n_aug_support_samples = 1\n",
    "    imagenet = CIFAR100(args, 'train')\n",
    "    print(len(imagenet))\n",
    "    print(imagenet.__getitem__(500)[0].shape)\n",
    "\n",
    "    metaimagenet = MetaCIFAR100(args, 'train')\n",
    "    print(len(metaimagenet))\n",
    "    print(metaimagenet.__getitem__(500)[0].size())\n",
    "    print(metaimagenet.__getitem__(500)[1].shape)\n",
    "    print(metaimagenet.__getitem__(500)[2].size())\n",
    "    print(metaimagenet.__getitem__(500)[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T21:24:18.321502Z",
     "iopub.status.busy": "2022-06-27T21:24:18.320816Z",
     "iopub.status.idle": "2022-06-27T21:24:18.327382Z",
     "shell.execute_reply": "2022-06-27T21:24:18.326331Z",
     "shell.execute_reply.started": "2022-06-27T21:24:18.321467Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wandb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T20:54:03.086568Z",
     "iopub.status.busy": "2022-08-18T20:54:03.085912Z",
     "iopub.status.idle": "2022-08-18T21:04:26.108993Z",
     "shell.execute_reply": "2022-08-18T21:04:26.107342Z",
     "shell.execute_reply.started": "2022-08-18T20:54:03.086529Z"
    }
   },
   "outputs": [],
   "source": [
    "#Resnet_12\n",
    "!python /kaggle/working/SKD/train_selfsupervison.py --model resnet12_ssl --model_path save/CIFAR-FS --dataset CIFAR-FS --data_root /kaggle/input/cifar-fs --epochs 65 --lr_decay_epochs 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
