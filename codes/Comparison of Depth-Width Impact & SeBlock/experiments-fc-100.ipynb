{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T09:57:39.094302Z",
     "iopub.status.busy": "2022-08-09T09:57:39.093613Z",
     "iopub.status.idle": "2022-08-09T09:57:41.916750Z",
     "shell.execute_reply": "2022-08-09T09:57:41.915489Z",
     "shell.execute_reply.started": "2022-08-09T09:57:39.094183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SKD'...\n",
      "remote: Enumerating objects: 96, done.\u001b[K\n",
      "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 96 (delta 38), reused 49 (delta 9), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (96/96), 2.30 MiB | 3.71 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/brjathu/SKD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T09:57:41.920192Z",
     "iopub.status.busy": "2022-08-09T09:57:41.919478Z",
     "iopub.status.idle": "2022-08-09T09:57:44.722133Z",
     "shell.execute_reply": "2022-08-09T09:57:44.721071Z",
     "shell.execute_reply.started": "2022-08-09T09:57:41.920151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb_api = \"xxxxxxxxxxxxxxxx\"\n",
    "\n",
    "wandb.login(key=wandb_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T09:57:46.614619Z",
     "iopub.status.busy": "2022-08-09T09:57:46.614171Z",
     "iopub.status.idle": "2022-08-09T09:58:01.808640Z",
     "shell.execute_reply": "2022-08-09T09:58:01.807348Z",
     "shell.execute_reply.started": "2022-08-09T09:57:46.614580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.18)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.5.12)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n",
      "Installing collected packages: wandb\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.12.18\n",
      "    Uninstalling wandb-0.12.18:\n",
      "      Successfully uninstalled wandb-0.12.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 2.9.3 requires wandb<0.13.0,>=0.10.0, but you have wandb 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed wandb-0.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T09:58:01.811872Z",
     "iopub.status.busy": "2022-08-09T09:58:01.811456Z",
     "iopub.status.idle": "2022-08-09T09:58:01.819114Z",
     "shell.execute_reply": "2022-08-09T09:58:01.818034Z",
     "shell.execute_reply.started": "2022-08-09T09:58:01.811832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T09:58:01.821957Z",
     "iopub.status.busy": "2022-08-09T09:58:01.821156Z",
     "iopub.status.idle": "2022-08-09T09:58:01.831625Z",
     "shell.execute_reply": "2022-08-09T09:58:01.830596Z",
     "shell.execute_reply.started": "2022-08-09T09:58:01.821916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/SKD\n"
     ]
    }
   ],
   "source": [
    "cd ./SKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T09:58:01.836081Z",
     "iopub.status.busy": "2022-08-09T09:58:01.834930Z",
     "iopub.status.idle": "2022-08-09T10:00:30.039273Z",
     "shell.execute_reply": "2022-08-09T10:00:30.037704Z",
     "shell.execute_reply.started": "2022-08-09T09:58:01.836043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.2.1\n",
      "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mkl==2019.0\n",
      "  Downloading mkl-2019.0-py2.py3-none-manylinux1_x86_64.whl (261.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.18.4\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Pillow==7.1.2\n",
      "  Downloading Pillow-7.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit_learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch==1.5.0\n",
      "  Downloading torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.0/752.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.6.0\n",
      "  Downloading torchvision-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting tqdm==4.46.0\n",
      "  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb==0.8.36\n",
      "  Downloading wandb-0.8.36-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 1)) (2.8.2)\n",
      "Collecting intel-openmp\n",
      "  Downloading intel_openmp-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (10.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn==0.23.1->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn==0.23.1->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.5.0->-r requirements.txt (line 7)) (0.18.2)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (1.5.12)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (3.1.27)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (2.27.1)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (5.9.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (6.0)\n",
      "Collecting watchdog>=0.8.3\n",
      "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (1.0.9)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (5.2.0)\n",
      "Collecting gql==0.2.0\n",
      "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.8.36->-r requirements.txt (line 10)) (8.0.4)\n",
      "Collecting graphql-core<2,>=0.5.0\n",
      "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from gql==0.2.0->wandb==0.8.36->-r requirements.txt (line 10)) (2.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click>=7.0->wandb==0.8.36->-r requirements.txt (line 10)) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (4.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (2022.6.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.8.36->-r requirements.txt (line 10)) (3.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click>=7.0->wandb==0.8.36->-r requirements.txt (line 10)) (3.8.0)\n",
      "Building wheels for collected packages: gql, nvidia-ml-py3, subprocess32, graphql-core\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=7638 sha256=c0afc818616a656eb3b92ca6441ed70319a88115993cf57b3e83748fad0655cf\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=f05f2bb0c8fb17c0e9b985942cb4a45647e8c905781af016830272175991b85f\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=f471a4bd60aafd417decc997ce6ea6286eb189c46be4147ecb6bbd6c1c6dd72e\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104649 sha256=aa81c0e2fc8ffc086edb34c3da527c714272474870a3536f36b284700da2c6e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
      "Successfully built gql nvidia-ml-py3 subprocess32 graphql-core\n",
      "Installing collected packages: nvidia-ml-py3, intel-openmp, watchdog, tqdm, subprocess32, Pillow, numpy, mkl, torch, scipy, matplotlib, graphql-core, torchvision, scikit_learn, gql, wandb\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.1.1\n",
      "    Uninstalling Pillow-9.1.1:\n",
      "      Successfully uninstalled Pillow-9.1.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0\n",
      "    Uninstalling torch-1.11.0:\n",
      "      Successfully uninstalled torch-1.11.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.12.0\n",
      "    Uninstalling torchvision-0.12.0:\n",
      "      Successfully uninstalled torchvision-0.12.0\n",
      "  Attempting uninstall: scikit_learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.1\n",
      "    Uninstalling wandb-0.13.1:\n",
      "      Successfully uninstalled wandb-0.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\n",
      "dask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\n",
      "beatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.1 which is incompatible.\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.4 which is incompatible.\n",
      "wfdb 3.4.1 requires matplotlib>=3.3.4, but you have matplotlib 3.2.1 which is incompatible.\n",
      "tfx-bsl 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow 2.6.4 requires absl-py~=0.10, but you have absl-py 1.0.0 which is incompatible.\n",
      "tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.18.4 which is incompatible.\n",
      "tensorflow 2.6.4 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\n",
      "tensorflow 2.6.4 requires wrapt~=1.12.1, but you have wrapt 1.14.1 which is incompatible.\n",
      "tensorflow-transform 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.4 which is incompatible.\n",
      "stumpy 1.11.1 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
      "sklearn-pandas 2.2.0 requires scipy>=1.5.1, but you have scipy 1.4.1 which is incompatible.\n",
      "pytorch-lightning 1.6.4 requires torch>=1.8.*, but you have torch 1.5.0 which is incompatible.\n",
      "pytorch-lightning 1.6.4 requires tqdm>=4.57.0, but you have tqdm 4.46.0 which is incompatible.\n",
      "pytesseract 0.3.9 requires Pillow>=8.0.0, but you have pillow 7.1.2 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n",
      "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.4 which is incompatible.\n",
      "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
      "phik 0.12.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.2.1 which is incompatible.\n",
      "panel 0.13.1 requires tqdm>=4.48.0, but you have tqdm 4.46.0 which is incompatible.\n",
      "osmnx 1.1.1 requires matplotlib>=3.3, but you have matplotlib 3.2.1 which is incompatible.\n",
      "osmnx 1.1.1 requires numpy>=1.19, but you have numpy 1.18.4 which is incompatible.\n",
      "nilearn 0.9.1 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
      "mlxtend 0.20.0 requires scikit-learn>=1.0.2, but you have scikit-learn 0.23.1 which is incompatible.\n",
      "kornia 0.5.8 requires torch>=1.6.0, but you have torch 1.5.0 which is incompatible.\n",
      "kaggle-environments 1.9.10 requires numpy>=1.19.5, but you have numpy 1.18.4 which is incompatible.\n",
      "jaxlib 0.3.10+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.4 which is incompatible.\n",
      "jax 0.3.13 requires numpy>=1.19, but you have numpy 1.18.4 which is incompatible.\n",
      "imbalanced-learn 0.9.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.23.1 which is incompatible.\n",
      "imageio 2.19.2 requires pillow>=8.3.2, but you have pillow 7.1.2 which is incompatible.\n",
      "hypertools 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\n",
      "gplearn 0.4.2 requires scikit-learn>=1.0.2, but you have scikit-learn 0.23.1 which is incompatible.\n",
      "flax 0.5.1 requires rich~=11.1.0, but you have rich 12.4.4 which is incompatible.\n",
      "featuretools 1.9.2 requires numpy>=1.21.0, but you have numpy 1.18.4 which is incompatible.\n",
      "fastai 2.6.3 requires torch<1.12,>=1.7.0, but you have torch 1.5.0 which is incompatible.\n",
      "fastai 2.6.3 requires torchvision>=0.8.2, but you have torchvision 0.6.0 which is incompatible.\n",
      "fairscale 0.4.6 requires torch>=1.8.0, but you have torch 1.5.0 which is incompatible.\n",
      "explainable-ai-sdk 1.3.3 requires matplotlib>=3.2.2, but you have matplotlib 3.2.1 which is incompatible.\n",
      "datasets 2.1.0 requires tqdm>=4.62.1, but you have tqdm 4.46.0 which is incompatible.\n",
      "dask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\n",
      "dask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\n",
      "apache-beam 2.39.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\n",
      "apache-beam 2.39.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\n",
      "allennlp 2.9.3 requires torch<1.12.0,>=1.6.0, but you have torch 1.5.0 which is incompatible.\n",
      "allennlp 2.9.3 requires torchvision<0.13.0,>=0.8.1, but you have torchvision 0.6.0 which is incompatible.\n",
      "allennlp 2.9.3 requires tqdm>=4.62, but you have tqdm 4.46.0 which is incompatible.\n",
      "allennlp 2.9.3 requires wandb<0.13.0,>=0.10.0, but you have wandb 0.8.36 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Pillow-7.1.2 gql-0.2.0 graphql-core-1.1 intel-openmp-2022.1.0 matplotlib-3.2.1 mkl-2019.0 numpy-1.18.4 nvidia-ml-py3-7.352.0 scikit_learn-0.23.1 scipy-1.4.1 subprocess32-3.5.4 torch-1.5.0 torchvision-0.6.0 tqdm-4.46.0 wandb-0.8.36 watchdog-2.1.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T10:00:30.043738Z",
     "iopub.status.busy": "2022-08-09T10:00:30.043242Z",
     "iopub.status.idle": "2022-08-09T10:01:31.433617Z",
     "shell.execute_reply": "2022-08-09T10:01:31.432373Z",
     "shell.execute_reply.started": "2022-08-09T10:00:30.043686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - mkl-service\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    mkl-service-2.4.0          |   py37hc4a68ce_0          61 KB  conda-forge\n",
      "    openssl-1.1.1q             |       h166bdaf_0         2.1 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  mkl-service        conda-forge/linux-64::mkl-service-2.4.0-py37hc4a68ce_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1q       | 2.1 MB    | ##################################### | 100% \n",
      "mkl-service-2.4.0    | 61 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install mkl-service -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T10:01:31.438538Z",
     "iopub.status.busy": "2022-08-09T10:01:31.437626Z",
     "iopub.status.idle": "2022-08-09T10:01:31.448691Z",
     "shell.execute_reply": "2022-08-09T10:01:31.445436Z",
     "shell.execute_reply.started": "2022-08-09T10:01:31.438497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SKD/models/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SKD/models/__init__.py\n",
    "\n",
    "from .resnet_ssl import resnet12_ssl,resnet50,resnet101,seresnet12,seresnet50,seresnet101\n",
    "\n",
    "\n",
    "model_pool = [\n",
    "    'resnet12_ssl',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'seresnet12',\n",
    "    'seresnet50',\n",
    "    'seresnet101'\n",
    "]\n",
    "\n",
    "model_dict = {\n",
    "    'resnet12_ssl': resnet12_ssl,\n",
    "    'resnet50': resnet50,\n",
    "    'resnet101': resnet101,\n",
    "    'seresnet12': seresnet12,\n",
    "    'seresnet50': seresnet50,\n",
    "    'seresnet101':seresnet101\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T10:09:35.621431Z",
     "iopub.status.busy": "2022-08-09T10:09:35.621024Z",
     "iopub.status.idle": "2022-08-09T10:09:35.637780Z",
     "shell.execute_reply": "2022-08-09T10:09:35.636540Z",
     "shell.execute_reply.started": "2022-08-09T10:09:35.621376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SKD/train_selfsupervison.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SKD/train_selfsupervison.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import mkl\n",
    "\n",
    "# import tensorboard_logger as tb_logger\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import model_pool\n",
    "from models.util import create_model\n",
    "\n",
    "from dataset.mini_imagenet import ImageNet, MetaImageNet\n",
    "from dataset.tiered_imagenet import TieredImageNet, MetaTieredImageNet\n",
    "from dataset.cifar import CIFAR100, MetaCIFAR100\n",
    "from dataset.transform_cfg import transforms_options, transforms_test_options, transforms_list\n",
    "\n",
    "from util import adjust_learning_rate, accuracy, AverageMeter\n",
    "from eval.meta_eval import meta_test, meta_test_tune\n",
    "from eval.cls_eval import validate\n",
    "\n",
    "from models.resnet import resnet12\n",
    "import numpy as np\n",
    "from util import Logger\n",
    "import wandb\n",
    "from dataloader import get_dataloaders\n",
    "\n",
    "def get_freer_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    return np.argmax(memory_available)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(get_freer_gpu())\n",
    "mkl.set_num_threads(2)\n",
    "\n",
    "\n",
    "def parse_option():\n",
    "\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--eval_freq', type=int, default=10, help='meta-eval frequency')\n",
    "    parser.add_argument('--print_freq', type=int, default=100, help='print frequency')\n",
    "    parser.add_argument('--tb_freq', type=int, default=500, help='tb frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=10, help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,80', help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "    parser.add_argument('--adam', action='store_true', help='use adam optimizer')\n",
    "    parser.add_argument('--simclr', type=bool, default=False, help='use simple contrastive learning representation')\n",
    "    parser.add_argument('--ssl', type=bool, default=True, help='use self supervised learning')\n",
    "    parser.add_argument('--tags', type=str, default=\"gen0, ssl\", help='add tags for the experiment')\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet12', choices=model_pool)\n",
    "    parser.add_argument('--dataset', type=str, default='miniImageNet', choices=['miniImageNet', 'tieredImageNet',\n",
    "                                                                                'CIFAR-FS', 'FC100'])\n",
    "    parser.add_argument('--transform', type=str, default='A', choices=transforms_list)\n",
    "    parser.add_argument('--use_trainval', type=bool, help='use trainval set')\n",
    "\n",
    "    # cosine annealing\n",
    "    parser.add_argument('--cosine', action='store_true', help='using cosine annealing')\n",
    "\n",
    "    # specify folder\n",
    "    parser.add_argument('--model_path', type=str, default='save/', help='path to save model')\n",
    "    parser.add_argument('--tb_path', type=str, default='tb/', help='path to tensorboard')\n",
    "    parser.add_argument('--data_root', type=str, default='/raid/data/IncrementLearn/imagenet/Datasets/MiniImagenet/', help='path to data root')\n",
    "\n",
    "    # meta setting\n",
    "    parser.add_argument('--n_test_runs', type=int, default=600, metavar='N',\n",
    "                        help='Number of test runs')\n",
    "    parser.add_argument('--n_ways', type=int, default=5, metavar='N',\n",
    "                        help='Number of classes for doing each classification run')\n",
    "    parser.add_argument('--n_shots', type=int, default=1, metavar='N',\n",
    "                        help='Number of shots in test')\n",
    "    parser.add_argument('--n_queries', type=int, default=15, metavar='N',\n",
    "                        help='Number of query in test')\n",
    "    parser.add_argument('--n_aug_support_samples', default=5, type=int,\n",
    "                        help='The number of augmented samples for each meta test sample')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=1, metavar='test_batch_size',\n",
    "                        help='Size of test batch)')\n",
    "\n",
    "    parser.add_argument('-t', '--trial', type=str, default='1', help='the experiment id')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #hyper parameters\n",
    "    parser.add_argument('--gamma', type=float, default=2, help='loss cofficient for ssl loss')\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    if opt.dataset == 'CIFAR-FS' or opt.dataset == 'FC100':\n",
    "        opt.transform = 'D'\n",
    "\n",
    "    if opt.use_trainval:\n",
    "        opt.trial = opt.trial + '_trainval'\n",
    "\n",
    "    # set the path according to the environment\n",
    "    if not opt.model_path:\n",
    "        opt.model_path = './models_pretrained'\n",
    "    if not opt.tb_path:\n",
    "        opt.tb_path = './tensorboard'\n",
    "    if not opt.data_root:\n",
    "        opt.data_root = './data/{}'.format(opt.dataset)\n",
    "    else:\n",
    "        opt.data_root = '{}/{}'.format(opt.data_root, opt.dataset)\n",
    "    opt.data_aug = True\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "        \n",
    "    tags = opt.tags.split(',')\n",
    "    opt.tags = list([])\n",
    "    for it in tags:\n",
    "        opt.tags.append(it)\n",
    "\n",
    "    opt.model_name = '{}_{}_lr_{}_decay_{}_trans_{}'.format(opt.model, opt.dataset, opt.learning_rate,\n",
    "                                                            opt.weight_decay, opt.transform)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    if opt.adam:\n",
    "        opt.model_name = '{}_useAdam'.format(opt.model_name)\n",
    "\n",
    "    opt.model_name = '{}_trial_{}'.format(opt.model_name, opt.trial)\n",
    "\n",
    "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.tb_folder):\n",
    "        os.makedirs(opt.tb_folder)\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "\n",
    "    opt.n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    \n",
    "    #extras\n",
    "    opt.fresh_start = True\n",
    "    \n",
    "    \n",
    "    return opt\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    opt = parse_option()\n",
    "    wandb.init(project=opt.model_path.split(\"/\")[-1], tags=opt.tags)\n",
    "    wandb.config.update(opt)\n",
    "    wandb.save('*.py')\n",
    "    wandb.run.save()\n",
    "    \n",
    "        \n",
    "    train_loader, val_loader, meta_testloader, meta_valloader, n_cls = get_dataloaders(opt)\n",
    "\n",
    "    # model\n",
    "    model = create_model(opt.model, n_cls, opt.dataset)\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # optimizer\n",
    "    if opt.adam:\n",
    "        print(\"Adam\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=opt.learning_rate,\n",
    "                                     weight_decay=0.0005)\n",
    "    else:\n",
    "        print(\"SGD\")\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=opt.learning_rate,\n",
    "                              momentum=opt.momentum,\n",
    "                              weight_decay=opt.weight_decay)\n",
    "        \n",
    "        \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if opt.n_gpu > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # set cosine annealing scheduler\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.epochs, eta_min, -1)\n",
    "\n",
    "    # routine: supervised pre-training\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "            if opt.cosine:\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                adjust_learning_rate(epoch, opt, optimizer)\n",
    "            print(\"==> training...\")\n",
    "\n",
    "\n",
    "            time1 = time.time()\n",
    "            train_acc, train_acc5, train_loss = train(epoch, train_loader, model, criterion, optimizer, opt)\n",
    "            time2 = time.time()\n",
    "            print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "\n",
    "\n",
    "            val_acc, val_acc_top5, val_loss = validate(val_loader, model, criterion, opt)\n",
    "\n",
    "\n",
    "            #validate\n",
    "            start = time.time()\n",
    "            meta_val_acc, meta_val_std, meta_val_acc5, meta_val_std5 = meta_test(model, meta_valloader,use_logit=True)\n",
    "            test_time = time.time() - start\n",
    "            print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}, Time: {:.1f}'.format(meta_val_acc, meta_val_std, test_time))\n",
    "\n",
    "            #evaluate\n",
    "            start = time.time()\n",
    "            meta_test_acc, meta_test_std, meta_test_acc5, meta_test_std5 = meta_test(model, meta_testloader,use_logit=True)\n",
    "            test_time = time.time() - start\n",
    "            print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}, Time: {:.1f}'.format(meta_test_acc, meta_test_std, test_time))\n",
    "\n",
    "\n",
    "            # regular saving\n",
    "            if epoch % opt.save_freq == 0 or epoch==opt.epochs:\n",
    "                print('==> Saving...')\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model': model.state_dict(),\n",
    "                }            \n",
    "                save_file = os.path.join(opt.save_folder, 'model_'+str(wandb.run.name)+'.pth')\n",
    "                torch.save(state, save_file)\n",
    "\n",
    "                #wandb saving\n",
    "                torch.save(state, os.path.join(wandb.run.dir, \"model.pth\"))\n",
    "\n",
    "                ## onnx saving\n",
    "                #dummy_input = torch.autograd.Variable(torch.randn(1, 3, 32, 32)).cuda()\n",
    "                #torch.onnx.export(model, dummy_input, os.path.join(wandb.run.dir, \"model.onnx\"))\n",
    "\n",
    "            wandb.log({'epoch': epoch, \n",
    "                       'Train Acc': train_acc,\n",
    "                       'Train Acc top 5': train_acc5,\n",
    "                       'Train Loss': train_loss,\n",
    "                       'Val Acc': val_acc,\n",
    "                       'Val Acc top 5': val_acc_top5,\n",
    "                       'Val Loss': val_loss,\n",
    "                       'Meta Test Acc': meta_test_acc,\n",
    "                       'Meta Test std': meta_test_std,\n",
    "                       'Meta Val Acc': meta_val_acc,\n",
    "                       'Meta Val std': meta_val_std,\n",
    "                       'Meta Test Acc5': meta_test_acc5,\n",
    "                       'Meta Test std5': meta_test_std5,\n",
    "                       'Meta Val Acc5': meta_val_acc5,\n",
    "                       'Meta Val std5': meta_val_std5\n",
    "                      })\n",
    "\n",
    "    #final report \n",
    "    generate_final_report(model, opt, wandb)\n",
    "    \n",
    "    #remove output.txt log file \n",
    "    output_log_file = os.path.join(wandb.run.dir, \"output.log\")\n",
    "    if os.path.isfile(output_log_file):\n",
    "        os.remove(output_log_file)\n",
    "    else:    ## Show an error ##\n",
    "        print(\"Error: %s file not found\" % output_log_file)\n",
    "        \n",
    "        \n",
    "        \n",
    "def train(epoch, train_loader, model, criterion, optimizer, opt):\n",
    "    \"\"\"One epoch training\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "        for idx, (input, target, _) in enumerate(pbar):\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            input = input.float()\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            \n",
    "            batch_size = input.size()[0]\n",
    "            x = input\n",
    "            x_90 = x.transpose(2,3).flip(2)\n",
    "            x_180 = x.flip(2).flip(3)\n",
    "            x_270 = x.flip(2).transpose(2,3)\n",
    "            generated_data = torch.cat((x, x_90, x_180, x_270),0)\n",
    "            train_targets = target.repeat(4)\n",
    "            \n",
    "            rot_labels = torch.zeros(4*batch_size).cuda().long()\n",
    "            for i in range(4*batch_size):\n",
    "                if i < batch_size:\n",
    "                    rot_labels[i] = 0\n",
    "                elif i < 2*batch_size:\n",
    "                    rot_labels[i] = 1\n",
    "                elif i < 3*batch_size:\n",
    "                    rot_labels[i] = 2\n",
    "                else:\n",
    "                    rot_labels[i] = 3\n",
    "\n",
    "            # ===================forward=====================\n",
    "            \n",
    "            (_,_,_,_, feat), (train_logit, rot_logits) = model(generated_data, rot=True)\n",
    "            \n",
    "            rot_labels = F.one_hot(rot_labels.to(torch.int64), 4).float()\n",
    "            loss_ss = torch.sum(F.binary_cross_entropy_with_logits(input = rot_logits, target = rot_labels))\n",
    "            loss_ce = criterion(train_logit, train_targets)\n",
    "            \n",
    "            loss = opt.gamma * loss_ss + loss_ce\n",
    "            \n",
    "            acc1, acc5 = accuracy(train_logit, train_targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            # ===================backward=====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # ===================meters=====================\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            \n",
    "            pbar.set_postfix({\"Acc@1\":'{0:.2f}'.format(top1.avg.cpu().numpy()), \n",
    "                              \"Acc@5\":'{0:.2f}'.format(top5.avg.cpu().numpy(),2), \n",
    "                              \"Loss\" :'{0:.2f}'.format(losses.avg,2), \n",
    "                             })\n",
    "\n",
    "    print('Train_Acc@1 {top1.avg:.3f} Train_Acc@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg, losses.avg\n",
    "\n",
    "\n",
    "\n",
    "def generate_final_report(model, opt, wandb):\n",
    "    \n",
    "    \n",
    "    opt.n_shots = 1\n",
    "    train_loader, val_loader, meta_testloader, meta_valloader, _ = get_dataloaders(opt)\n",
    "    \n",
    "    #validate\n",
    "    meta_val_acc, meta_val_std, meta_val_acc5, meta_val_std5 = meta_test(model, meta_valloader, use_logit=True)\n",
    "    \n",
    "   # meta_val_acc_feat, meta_val_std_feat = meta_test(model, meta_valloader, use_logit=False)\n",
    "\n",
    "    #evaluate\n",
    "    meta_test_acc, meta_test_std, meta_test_acc5, meta_test_std5 = meta_test(model, meta_testloader, use_logit=True)\n",
    "    \n",
    "    #meta_test_acc_feat, meta_test_std_feat = meta_test(model, meta_testloader, use_logit=False)\n",
    "        \n",
    "    print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}'.format(meta_val_acc, meta_val_std))\n",
    "    #print('Meta Val Acc (feat): {:.4f}, Meta Val std (feat): {:.4f}'.format(meta_val_acc_feat, meta_val_std_feat))\n",
    "    print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}'.format(meta_test_acc, meta_test_std))\n",
    "    #print('Meta Test Acc (feat): {:.4f}, Meta Test std (feat): {:.4f}'.format(meta_test_acc_feat, meta_test_std_feat))\n",
    "    \n",
    "    \n",
    "    wandb.log({'Final Meta Test Acc @1': meta_test_acc,\n",
    "               'Final Meta Test std @1': meta_test_std,\n",
    "               'Final Meta Test Acc @5': meta_test_acc5,\n",
    "               'Final Meta Test std @5': meta_test_std5,\n",
    "               #'Final Meta Test Acc  (feat) @1': meta_test_acc_feat,\n",
    "               #'Final Meta Test std  (feat) @1': meta_test_std_feat,\n",
    "               'Final Meta Val Acc @1': meta_val_acc,\n",
    "               'Final Meta Val std @1': meta_val_std,\n",
    "               'Final Meta Val Acc @5': meta_val_acc5,\n",
    "               'Final Meta Val std @5': meta_val_std5,\n",
    "               #'Final Meta Val Acc   (feat) @1': meta_val_acc_feat,\n",
    "               #'Final Meta Val std   (feat) @1': meta_val_std_feat\n",
    "              })\n",
    "\n",
    "    \n",
    "   # opt.n_shots = 5\n",
    "   # train_loader, val_loader, meta_testloader, meta_valloader, _ = get_dataloaders(opt)\n",
    "    \n",
    "    #validate\n",
    "   # meta_val_acc, meta_val_std = meta_test(model, meta_valloader, use_logit=True)\n",
    "    \n",
    "    #meta_val_acc_feat, meta_val_std_feat = meta_test(model, meta_valloader, use_logit=False)\n",
    "\n",
    "    #evaluate\n",
    "   # meta_test_acc, meta_test_std = meta_test(model, meta_testloader, use_logit=True)\n",
    "    \n",
    "    #meta_test_acc_feat, meta_test_std_feat = meta_test(model, meta_testloader, use_logit=False)\n",
    "        \n",
    "   # print('Meta Val Acc : {:.4f}, Meta Val std: {:.4f}'.format(meta_val_acc, meta_val_std))\n",
    "    #print('Meta Val Acc (feat): {:.4f}, Meta Val std (feat): {:.4f}'.format(meta_val_acc_feat, meta_val_std_feat))\n",
    "   # print('Meta Test Acc: {:.4f}, Meta Test std: {:.4f}'.format(meta_test_acc, meta_test_std))\n",
    "    #print('Meta Test Acc (feat): {:.4f}, Meta Test std (feat): {:.4f}'.format(meta_test_acc_feat, meta_test_std_feat))\n",
    "\n",
    "    #wandb.log({'Final Meta Test Acc @5': meta_test_acc,\n",
    "     #          'Final Meta Test std @5': meta_test_std,\n",
    "               #'Final Meta Test Acc  (feat) @5': meta_test_acc_feat,\n",
    "               #'Final Meta Test std  (feat) @5': meta_test_std_feat,\n",
    "       #        'Final Meta Val Acc @5': meta_val_acc,\n",
    "      #         'Final Meta Val std @5': meta_val_std,\n",
    "               #'Final Meta Val Acc   (feat) @5': meta_val_acc_feat,\n",
    "               #'Final Meta Val std   (feat) @5': meta_val_std_feat\n",
    "        #      })\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T10:09:36.734256Z",
     "iopub.status.busy": "2022-08-09T10:09:36.733864Z",
     "iopub.status.idle": "2022-08-09T10:09:36.750443Z",
     "shell.execute_reply": "2022-08-09T10:09:36.749314Z",
     "shell.execute_reply.started": "2022-08-09T10:09:36.734222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SKD/eval/meta_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SKD/eval/meta_eval.py \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import t\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys, os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from util import accuracy\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, data5, confidence=0.95):\n",
    "    a = 100.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * t._ppf((1+confidence)/2., n-1)\n",
    "    \n",
    "    a5 = 100.0 * np.array(data5)\n",
    "    n5 = len(a5)\n",
    "    m5, se5 = np.mean(a5), scipy.stats.sem(a5)\n",
    "    h5 = se5 * t._ppf((1+confidence)/2., n5-1)\n",
    "    return m, h, m5, h5\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    norm = x.pow(2).sum(1, keepdim=True).pow(1. / 2)\n",
    "    out = x.div(norm)\n",
    "    return out\n",
    "\n",
    "\n",
    "def meta_test(net, testloader, use_logit=False, is_norm=True, classifier='LR'):\n",
    "    net = net.eval()\n",
    "    acc = []\n",
    "    acc5 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "            for idx, data in enumerate(pbar):\n",
    "                support_xs, support_ys, support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5 = data\n",
    "\n",
    "                support_xs = support_xs.cuda()\n",
    "                support_xs5 = support_xs5.cuda()\n",
    "                query_xs = query_xs.cuda()\n",
    "                query_xs5 = query_xs5.cuda()\n",
    "                batch_size, _, height, width, channel = support_xs.size()\n",
    "                batch_size5, _, height5, width5, channel5 = support_xs5.size()\n",
    "                support_xs = support_xs.view(-1, height, width, channel)\n",
    "                support_xs5 = support_xs5.view(-1, height5, width5, channel5)\n",
    "                query_xs = query_xs.view(-1, height, width, channel)\n",
    "                query_xs5 = query_xs5.view(-1, height5, width5, channel5)\n",
    "\n",
    "                \n",
    "                \n",
    "#                 batch_size = support_xs.size()[0]\n",
    "#                 x = support_xs\n",
    "#                 x_90 = x.transpose(2,3).flip(2)\n",
    "#                 x_180 = x.flip(2).flip(3)\n",
    "#                 x_270 = x.flip(2).transpose(2,3)\n",
    "#                 generated_data = torch.cat((x, x_90, x_180, x_270),0)\n",
    "#                 support_ys = support_ys.repeat(1,4)\n",
    "#                 support_xs = generated_data\n",
    "            \n",
    "#                 print(support_xs.size())\n",
    "#                 print(support_ys.size())\n",
    "\n",
    "\n",
    "\n",
    "                if use_logit:\n",
    "                    support_features = net(support_xs).view(support_xs.size(0), -1)\n",
    "                    support_features5 = net(support_xs5).view(support_xs5.size(0), -1)\n",
    "                    query_features = net(query_xs).view(query_xs.size(0), -1)\n",
    "                    query_features5 = net(query_xs5).view(query_xs5.size(0), -1)\n",
    "                else:\n",
    "                    feat_support, _ = net(support_xs, is_feat=True)\n",
    "                    support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                    feat_query, _ = net(query_xs, is_feat=True)\n",
    "                    query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "#                     feat_support, _ = net(support_xs)\n",
    "#                     support_features = feat_support.view(support_xs.size(0), -1)\n",
    "#                     feat_query, _ = net(query_xs)\n",
    "#                     query_features = feat_query.view(query_xs.size(0), -1)\n",
    "\n",
    "\n",
    "                if is_norm:\n",
    "                    support_features = normalize(support_features)\n",
    "                    support_features5 = normalize(support_features5)\n",
    "                    query_features = normalize(query_features)\n",
    "                    query_features5 = normalize(query_features5)\n",
    "\n",
    "                support_features = support_features.detach().cpu().numpy()\n",
    "                support_features5 = support_features5.detach().cpu().numpy()\n",
    "                query_features = query_features.detach().cpu().numpy()\n",
    "                query_features5 = query_features5.detach().cpu().numpy()\n",
    "                \n",
    "                support_ys = support_ys.view(-1).numpy()\n",
    "                support_ys5 = support_ys5.view(-1).numpy()\n",
    "                query_ys = query_ys.view(-1).numpy()\n",
    "                query_ys5 = query_ys5.view(-1).numpy()\n",
    "                \n",
    "                \n",
    "                \n",
    "                if classifier == 'LR':\n",
    "                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                                             multi_class='multinomial')\n",
    "                    clf5 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                                             multi_class='multinomial')                         \n",
    "                    clf.fit(support_features, support_ys)\n",
    "                    clf5.fit(support_features5, support_ys5)\n",
    "                    query_ys_pred = clf.predict(query_features)\n",
    "                    query_ys_pred5 = clf5.predict(query_features5)\n",
    "                elif classifier == 'NN':\n",
    "                    query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "                elif classifier == 'Cosine':\n",
    "                    query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "                else:\n",
    "                    raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "                    \n",
    "#                 bs = query_features.shape[0]//opt.n_aug_support_samples\n",
    "#                 a = np.reshape(query_ys_pred[:bs], (-1,1))\n",
    "#                 c = query_ys[:bs]\n",
    "#                 for i in range(1,opt.n_aug_support_samples):\n",
    "#                     a = np.hstack([a, np.reshape(query_ys_pred[i*bs:(i+1)*bs], (-1,1))])\n",
    "                \n",
    "#                 d = [] \n",
    "#                 for i in range(a.shape[0]):\n",
    "#                     b = Counter(a[i,:])\n",
    "#                     d.append(b.most_common(1)[0][0])\n",
    "                \n",
    "# #                 (values,counts) = np.unique(a,axis=1, return_counts=True)\n",
    "# #                 print(counts)\n",
    "# # ind=np.argmax(counts)\n",
    "# # print values[ind]  # pr\n",
    "\n",
    "\n",
    "# # #                 a = np.argmax\n",
    "# #                 print(a.shape)\n",
    "# #                 print(c.shape)\n",
    "                    \n",
    "                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))\n",
    "                acc5.append(metrics.accuracy_score(query_ys5, query_ys_pred5))\n",
    "                \n",
    "                pbar.set_postfix({\"FSL_Acc\":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})\n",
    "    \n",
    "    return mean_confidence_interval(acc,acc5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def meta_test_tune(net, testloader, use_logit=False, is_norm=True, classifier='LR', lamda=0.2):\n",
    "    net = net.eval()\n",
    "    acc = []\n",
    "    \n",
    "    with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "        for idx, data in enumerate(pbar):\n",
    "            support_xs, support_ys, query_xs, query_ys, support_ts, query_ts = data\n",
    "\n",
    "            support_xs = support_xs.cuda()\n",
    "            support_ys = support_ys.cuda()\n",
    "            query_ys = query_ys.cuda()\n",
    "            query_xs = query_xs.cuda()\n",
    "            batch_size, _, height, width, channel = support_xs.size()\n",
    "            support_xs = support_xs.view(-1, height, width, channel)\n",
    "            support_ys = support_ys.view(-1,1)\n",
    "            query_ys = query_ys.view(-1)\n",
    "            query_xs = query_xs.view(-1, height, width, channel)\n",
    "\n",
    "            if use_logit:\n",
    "                support_features = net(support_xs).view(support_xs.size(0), -1)\n",
    "                query_features = net(query_xs).view(query_xs.size(0), -1)\n",
    "            else:\n",
    "                feat_support, _ = net(support_xs, is_feat=True)\n",
    "                support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                feat_query, _ = net(query_xs, is_feat=True)\n",
    "                query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "            if is_norm:\n",
    "                support_features = normalize(support_features)\n",
    "                query_features = normalize(query_features)\n",
    "               \n",
    "            y_onehot = torch.FloatTensor(support_ys.size()[0], 5).cuda()\n",
    "\n",
    "            # In your for loop\n",
    "            y_onehot.zero_()\n",
    "            y_onehot.scatter_(1, support_ys, 1)\n",
    "\n",
    "    \n",
    "            X = support_features\n",
    "            XTX = torch.matmul(torch.t(X),X)\n",
    "            \n",
    "            B = torch.matmul( (XTX + lamda*torch.eye(640).cuda() ).inverse(), torch.matmul(torch.t(X), y_onehot.float()) )\n",
    "#             print(B.size())\n",
    "            m = nn.Sigmoid()\n",
    "            Y_pred = m(torch.matmul(query_features, B))\n",
    "                \n",
    "                \n",
    "#             print(Y_pred, query_ys)\n",
    "#             model = nn.Sequential(nn.Linear(64, 10),nn.LogSoftmax(dim=1))\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#             model.cuda()\n",
    "#             criterion.cuda()\n",
    "#             model.train()\n",
    "            \n",
    "#             for i in range(5):\n",
    "#                 output = model(support_features)\n",
    "#                 loss = criterion(output, support_ys)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward(retain_graph=True) # auto-grad \n",
    "#                 optimizer.step() # update  weights \n",
    "            \n",
    "#             model.eval()\n",
    "#             query_ys_pred = model(query_features)\n",
    "\n",
    "            acc1, acc5 = accuracy(Y_pred, query_ys, topk=(1, 1))\n",
    "            \n",
    "            \n",
    "#             support_features = support_features.detach().cpu().numpy()\n",
    "#             query_features = query_features.detach().cpu().numpy()\n",
    "\n",
    "#             support_ys = support_ys.view(-1).numpy()\n",
    "#             query_ys = query_ys.view(-1).numpy()\n",
    "\n",
    "#             if classifier == 'LR':\n",
    "#                 clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000,\n",
    "#                                          multi_class='multinomial')\n",
    "#                 clf.fit(support_features, support_ys)\n",
    "#                 query_ys_pred = clf.predict(query_features)\n",
    "#             elif classifier == 'NN':\n",
    "#                 query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "#             elif classifier == 'Cosine':\n",
    "#                 query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "#             else:\n",
    "#                 raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "            acc.append(acc1.item()/100.0)\n",
    "\n",
    "            pbar.set_postfix({\"FSL_Acc\":'{0:.4f}'.format(np.mean(acc))})\n",
    "                \n",
    "                \n",
    "    return mean_confidence_interval(acc)\n",
    "\n",
    "\n",
    "\n",
    "def meta_test_ensamble(net, testloader, use_logit=True, is_norm=True, classifier='LR'):\n",
    "    for n in net:\n",
    "        n = n.eval()\n",
    "    acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader, total=len(testloader)) as pbar:\n",
    "            for idx, data in enumerate(pbar):\n",
    "                support_xs, support_ys, query_xs, query_ys = data\n",
    "\n",
    "                support_xs = support_xs.cuda()\n",
    "                query_xs = query_xs.cuda()\n",
    "                batch_size, _, height, width, channel = support_xs.size()\n",
    "                support_xs = support_xs.view(-1, height, width, channel)\n",
    "                query_xs = query_xs.view(-1, height, width, channel)\n",
    "\n",
    "                if use_logit:\n",
    "                    support_features = net[0](support_xs).view(support_xs.size(0), -1)\n",
    "                    query_features = net[0](query_xs).view(query_xs.size(0), -1)\n",
    "                    for n in net[1:]:\n",
    "                        support_features += n(support_xs).view(support_xs.size(0), -1)\n",
    "                        query_features += n(query_xs).view(query_xs.size(0), -1)\n",
    "                else:\n",
    "                    feat_support, _ = net(support_xs, is_feat=True)\n",
    "                    support_features = feat_support[-1].view(support_xs.size(0), -1)\n",
    "                    feat_query, _ = net(query_xs, is_feat=True)\n",
    "                    query_features = feat_query[-1].view(query_xs.size(0), -1)\n",
    "\n",
    "                if is_norm:\n",
    "                    support_features = normalize(support_features)\n",
    "                    query_features = normalize(query_features)\n",
    "\n",
    "                support_features = support_features.detach().cpu().numpy()\n",
    "                query_features = query_features.detach().cpu().numpy()\n",
    "\n",
    "                support_ys = support_ys.view(-1).numpy()\n",
    "                query_ys = query_ys.view(-1).numpy()\n",
    "\n",
    "                if classifier == 'LR':\n",
    "                    clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000,\n",
    "                                             multi_class='multinomial')\n",
    "                    clf.fit(support_features, support_ys)\n",
    "                    query_ys_pred = clf.predict(query_features)\n",
    "                elif classifier == 'NN':\n",
    "                    query_ys_pred = NN(support_features, support_ys, query_features)\n",
    "                elif classifier == 'Cosine':\n",
    "                    query_ys_pred = Cosine(support_features, support_ys, query_features)\n",
    "                else:\n",
    "                    raise NotImplementedError('classifier not supported: {}'.format(classifier))\n",
    "\n",
    "                acc.append(metrics.accuracy_score(query_ys, query_ys_pred))\n",
    "                \n",
    "                pbar.set_postfix({\"FSL_Acc\":'{0:.2f}'.format(metrics.accuracy_score(query_ys, query_ys_pred))})\n",
    "                \n",
    "    return mean_confidence_interval(acc)\n",
    "\n",
    "\n",
    "def NN(support, support_ys, query):\n",
    "    \"\"\"nearest classifier\"\"\"\n",
    "    support = np.expand_dims(support.transpose(), 0)\n",
    "    query = np.expand_dims(query, 2)\n",
    "\n",
    "    diff = np.multiply(query - support, query - support)\n",
    "    distance = diff.sum(1)\n",
    "    min_idx = np.argmin(distance, axis=1)\n",
    "    pred = [support_ys[idx] for idx in min_idx]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def Cosine(support, support_ys, query):\n",
    "    \"\"\"Cosine classifier\"\"\"\n",
    "    support_norm = np.linalg.norm(support, axis=1, keepdims=True)\n",
    "    support = support / support_norm\n",
    "    query_norm = np.linalg.norm(query, axis=1, keepdims=True)\n",
    "    query = query / query_norm\n",
    "\n",
    "    cosine_distance = query @ support.transpose()\n",
    "    max_idx = np.argmax(cosine_distance, axis=1)\n",
    "    pred = [support_ys[idx] for idx in max_idx]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T10:09:37.849346Z",
     "iopub.status.busy": "2022-08-09T10:09:37.848510Z",
     "iopub.status.idle": "2022-08-09T10:09:37.866491Z",
     "shell.execute_reply": "2022-08-09T10:09:37.865447Z",
     "shell.execute_reply.started": "2022-08-09T10:09:37.849302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/SKD/dataset/cifar.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/SKD/dataset/cifar.py \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CIFAR100(Dataset):\n",
    "    \"\"\"support FC100 and CIFAR-FS\"\"\"\n",
    "    def __init__(self, args, partition='train', pretrain=True, is_sample=False, k=4096,\n",
    "                 transform=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_root = args.data_root\n",
    "        self.partition = partition\n",
    "        self.data_aug = args.data_aug\n",
    "        self.mean = [0.5071, 0.4867, 0.4408]\n",
    "        self.std = [0.2675, 0.2565, 0.2761]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.pretrain = pretrain\n",
    "        self.simclr = args.simclr\n",
    "        \n",
    "        \n",
    "        if transform is None:\n",
    "            if self.partition == 'train' and self.data_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.pretrain:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        else:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        self.data = {}\n",
    "\n",
    "        with open(os.path.join(self.data_root, self.file_pattern % partition), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            self.imgs = data['data']\n",
    "            labels = data['labels']\n",
    "            # adjust sparse labels to labels from 0 to n.\n",
    "            cur_class = 0\n",
    "            label2label = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label not in label2label:\n",
    "                    label2label[label] = cur_class\n",
    "                    cur_class += 1\n",
    "            new_labels = []\n",
    "            for idx, label in enumerate(labels):\n",
    "                new_labels.append(label2label[label])\n",
    "            self.labels = new_labels\n",
    "        \n",
    "        \n",
    "        # pre-process for contrastive sampling\n",
    "        self.k = k\n",
    "        self.is_sample = is_sample\n",
    "        if self.is_sample:\n",
    "            self.labels = np.asarray(self.labels)\n",
    "            self.labels = self.labels - np.min(self.labels)\n",
    "            num_classes = np.max(self.labels) + 1\n",
    "\n",
    "            self.cls_positive = [[] for _ in range(num_classes)]\n",
    "            for i in range(len(self.imgs)):\n",
    "                self.cls_positive[self.labels[i]].append(i)\n",
    "\n",
    "            self.cls_negative = [[] for _ in range(num_classes)]\n",
    "            for i in range(num_classes):\n",
    "                for j in range(num_classes):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    self.cls_negative[i].extend(self.cls_positive[j])\n",
    "\n",
    "            self.cls_positive = [np.asarray(self.cls_positive[i]) for i in range(num_classes)]\n",
    "            self.cls_negative = [np.asarray(self.cls_negative[i]) for i in range(num_classes)]\n",
    "            self.cls_positive = np.asarray(self.cls_positive)\n",
    "            self.cls_negative = np.asarray(self.cls_negative)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = np.asarray(self.imgs[item]).astype('uint8')\n",
    "        target = self.labels[item] - min(self.labels)\n",
    "        \n",
    "        if(self.simclr):\n",
    "            img1 = self.transform(img)\n",
    "            img2 = self.transform(img)\n",
    "            return (img1, img2), target, item\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        if not self.is_sample:\n",
    "            return img, target, item\n",
    "        else:\n",
    "            pos_idx = item\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, item, sample_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CIFAR100_toy(Dataset):\n",
    "    \"\"\"support FC100 and CIFAR-FS\"\"\"\n",
    "    def __init__(self, args, partition='train', pretrain=True, is_sample=False, k=4096,\n",
    "                 transform=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_root = args.data_root\n",
    "        self.partition = partition\n",
    "        self.data_aug = args.data_aug\n",
    "        self.mean = [0.5071, 0.4867, 0.4408]\n",
    "        self.std = [0.2675, 0.2565, 0.2761]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.pretrain = pretrain\n",
    "        self.simclr = args.simclr\n",
    "        \n",
    "        \n",
    "        if transform is None:\n",
    "            if self.partition == 'train' and self.data_aug:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.pretrain:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        else:\n",
    "            self.file_pattern = '%s.pickle'\n",
    "        self.data = {}\n",
    "\n",
    "        with open(os.path.join(self.data_root, self.file_pattern % partition), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            self.imgs = data['data']\n",
    "            labels = data['labels']\n",
    "            # adjust sparse labels to labels from 0 to n.\n",
    "            cur_class = 0\n",
    "            label2label = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label not in label2label:\n",
    "                    label2label[label] = cur_class\n",
    "                    cur_class += 1\n",
    "            new_labels = []\n",
    "            for idx, label in enumerate(labels):\n",
    "                new_labels.append(label2label[label])\n",
    "            self.labels = new_labels\n",
    "        \n",
    "        self.labels = np.array(self.labels)\n",
    "        self.imgs = np.array(self.imgs)\n",
    "        print(self.labels.shape)\n",
    "        print(self.imgs.shape)\n",
    "        \n",
    "        loc = np.where(self.labels<5)[0]\n",
    "        self.labels = self.labels[loc]\n",
    "        self.imgs   = self.imgs[loc]\n",
    "        \n",
    "        \n",
    "        self.k = k\n",
    "        self.is_sample = is_sample\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = np.asarray(self.imgs[item]).astype('uint8')\n",
    "        target = self.labels[item] - min(self.labels)\n",
    "        \n",
    "        if(self.simclr):\n",
    "            img1 = self.transform(img)\n",
    "            img2 = self.transform(img)\n",
    "            return (img1, img2), target, item\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        if not self.is_sample:\n",
    "            return img, target, item\n",
    "        else:\n",
    "            pos_idx = item\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, item, sample_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class MetaCIFAR100(CIFAR100):\n",
    "\n",
    "    def __init__(self, args, partition='train', train_transform=None, test_transform=None, fix_seed=True):\n",
    "        super(MetaCIFAR100, self).__init__(args, partition, False)\n",
    "        self.fix_seed = fix_seed\n",
    "        self.n_ways = args.n_ways\n",
    "        self.n_shots = args.n_shots\n",
    "        self.n_queries = args.n_queries\n",
    "        self.classes = list(self.data.keys())\n",
    "        self.n_test_runs = args.n_test_runs\n",
    "        self.n_aug_support_samples = args.n_aug_support_samples\n",
    "        if train_transform is None:\n",
    "            self.train_transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                lambda x: np.asarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = train_transform\n",
    "\n",
    "        if test_transform is None:\n",
    "            self.test_transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "            ])\n",
    "        else:\n",
    "            self.test_transform = test_transform\n",
    "\n",
    "        self.data = {}\n",
    "        for idx in range(self.imgs.shape[0]):\n",
    "            if self.labels[idx] not in self.data:\n",
    "                self.data[self.labels[idx]] = []\n",
    "            self.data[self.labels[idx]].append(self.imgs[idx])\n",
    "        self.classes = list(self.data.keys())\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.fix_seed:\n",
    "            np.random.seed(item)\n",
    "        cls_sampled = np.random.choice(self.classes, self.n_ways, False)\n",
    "        \n",
    "        support_xs = []\n",
    "        support_ys = []\n",
    "        support_ts = []\n",
    "        query_xs = []\n",
    "        query_ys = []\n",
    "        query_ts = []\n",
    "        \n",
    "        support_xs5 = []\n",
    "        support_ys5 = []\n",
    "        support_ts5 = []\n",
    "        query_xs5 = []\n",
    "        query_ys5 = []\n",
    "        query_ts5 = []\n",
    "        \n",
    "        for idx, cls in enumerate(cls_sampled):\n",
    "            imgs = np.asarray(self.data[cls]).astype('uint8')\n",
    "            support_xs_ids_sampled = np.random.choice(range(imgs.shape[0]), self.n_shots, False)\n",
    "            support_xs.append(imgs[support_xs_ids_sampled])\n",
    "            support_ys.append([idx] * self.n_shots)\n",
    "            support_ts.append([cls] * self.n_shots)\n",
    "                 \n",
    "            support_xs_ids_sampled5 = np.random.choice(range(imgs.shape[0]), 5, False)\n",
    "            support_xs5.append(imgs[support_xs_ids_sampled5])\n",
    "            #support_xs5.append(imgs[support_xs_ids_sampled])\n",
    "            support_ys5.append([idx] * 5)\n",
    "            support_ts5.append([cls] * 5)\n",
    "            \n",
    "            query_xs_ids = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled)\n",
    "            query_xs_ids = np.random.choice(query_xs_ids, self.n_queries, False)\n",
    "            query_xs.append(imgs[query_xs_ids])\n",
    "            query_ys.append([idx] * query_xs_ids.shape[0])\n",
    "            query_ts.append([cls] * query_xs_ids.shape[0])\n",
    "            \n",
    "            query_xs_ids5 = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled5)\n",
    "            query_xs_ids5 = np.random.choice(query_xs_ids5, self.n_queries, False)\n",
    "            query_xs5.append(imgs[query_xs_ids5])\n",
    "            query_ys5.append([idx] * query_xs_ids5.shape[0])\n",
    "            query_ts5.append([cls] * query_xs_ids5.shape[0])\n",
    "            \n",
    "        support_xs, support_ys,support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5 = np.array(support_xs), np.array(support_ys),np.array(support_xs5), np.array(support_ys5), np.array(\n",
    "            query_xs), np.array(query_ys), np.array(query_xs5),np.array(query_ys5)\n",
    "        \n",
    "        support_ts,support_ts5, query_ts, query_ts5 = np.array(support_ts),np.array(support_ts5), np.array(query_ts),np.array(query_ts5)\n",
    "        \n",
    "        num_ways, n_queries_per_way, height, width, channel = query_xs.shape\n",
    "        query_xs = query_xs.reshape((num_ways * n_queries_per_way, height, width, channel))\n",
    "        query_ys = query_ys.reshape((num_ways * n_queries_per_way,))\n",
    "        query_ts = query_ts.reshape((num_ways * n_queries_per_way,))\n",
    "        \n",
    "        num_ways5, n_queries_per_way5, height5, width5, channel5 = query_xs5.shape\n",
    "        query_xs5 = query_xs5.reshape((num_ways5 * n_queries_per_way5, height5, width5, channel5))\n",
    "        query_ys5 = query_ys5.reshape((num_ways5 * n_queries_per_way5,))\n",
    "        query_ts5 = query_ts5.reshape((num_ways5 * n_queries_per_way5,))\n",
    "\n",
    "        support_xs = support_xs.reshape((-1, height, width, channel))\n",
    "        support_xs5 = support_xs5.reshape((-1, height5, width5, channel5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.n_aug_support_samples > 1:\n",
    "            support_xs = np.tile(support_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            support_ys = np.tile(support_ys.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            support_ts = np.tile(support_ts.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "            support_xs5 = np.tile(support_xs5, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            support_ys5 = np.tile(support_ys5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            support_ts5 = np.tile(support_ts5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "        \n",
    "        \n",
    "        support_xs = np.split(support_xs, support_xs.shape[0], axis=0)\n",
    "        support_xs5 = np.split(support_xs5, support_xs5.shape[0], axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        query_xs = query_xs.reshape((-1, height, width, channel))\n",
    "        query_xs5 = query_xs5.reshape((-1, height5, width5, channel5))\n",
    "        \n",
    "        if self.n_aug_support_samples > 1:\n",
    "            query_xs = np.tile(query_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            query_ys = np.tile(query_ys.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            query_ts = np.tile(query_ts.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "            query_xs5 = np.tile(query_xs5, (self.n_aug_support_samples, 1, 1, 1))\n",
    "            query_ys5 = np.tile(query_ys5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            query_ts5 = np.tile(query_ts5.reshape((-1,)), (self.n_aug_support_samples))\n",
    "            \n",
    "        query_xs = np.split(query_xs, query_xs.shape[0], axis=0)\n",
    "        query_xs5 = np.split(query_xs5, query_xs5.shape[0], axis=0)\n",
    "\n",
    "        support_xs = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs)))\n",
    "        support_xs5 = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs5)))\n",
    "        query_xs = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs)))\n",
    "        query_xs5 = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs5)))\n",
    "\n",
    "        return support_xs, support_ys, support_xs5, support_ys5, query_xs, query_ys, query_xs5, query_ys5\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_test_runs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = lambda x: None\n",
    "    args.n_ways = 5\n",
    "    args.n_shots = 1\n",
    "    args.n_queries = 12\n",
    "    # args.data_root = 'data'\n",
    "    args.data_root = '/home/yonglong/Downloads/FC100'\n",
    "    args.data_aug = True\n",
    "    args.n_test_runs = 5\n",
    "    args.n_aug_support_samples = 1\n",
    "    imagenet = CIFAR100(args, 'train')\n",
    "    print(len(imagenet))\n",
    "    print(imagenet.__getitem__(500)[0].shape)\n",
    "\n",
    "    metaimagenet = MetaCIFAR100(args, 'train')\n",
    "    print(len(metaimagenet))\n",
    "    print(metaimagenet.__getitem__(500)[0].size())\n",
    "    print(metaimagenet.__getitem__(500)[1].shape)\n",
    "    print(metaimagenet.__getitem__(500)[2].size())\n",
    "    print(metaimagenet.__getitem__(500)[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T21:02:11.111704Z",
     "iopub.status.busy": "2022-06-28T21:02:11.111296Z",
     "iopub.status.idle": "2022-06-28T21:02:11.117077Z",
     "shell.execute_reply": "2022-06-28T21:02:11.115796Z",
     "shell.execute_reply.started": "2022-06-28T21:02:11.111676Z"
    }
   },
   "outputs": [],
   "source": [
    "#!wandb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T10:10:47.186752Z",
     "iopub.status.busy": "2022-08-09T10:10:47.186307Z",
     "iopub.status.idle": "2022-08-09T10:21:23.463053Z",
     "shell.execute_reply": "2022-08-09T10:21:23.461884Z",
     "shell.execute_reply.started": "2022-08-09T10:10:47.186715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.8.36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.13.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20220809_101051-11gsx335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-morning-24\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/geolep/FC100\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/geolep/FC100/runs/11gsx335\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "*********** resnet12_ssl\n",
      "SGD\n",
      "==> training...\n",
      "  0%|                                                   | 0/750 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "100%|████▉| 749/750 [01:28<00:00, 10.89it/s, Acc@1=9.41, Acc@5=29.27, Loss=4.91]Train_Acc@1 9.414 Train_Acc@5 29.271\n",
      "100%|█████| 750/750 [01:28<00:00,  8.43it/s, Acc@1=9.41, Acc@5=29.27, Loss=4.91]\n",
      "epoch 1, total time 88.98\n",
      "100%|██| 1125/1125 [00:33<00:00, 33.26it/s, Acc@1=18.23, Acc@5=18.23, Loss=3.24]\n",
      "Val_Acc@1 18.233 Val_Acc@5 47.322\n",
      "  0%|                                                   | 0/600 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "100%|███████████████████████████| 600/600 [02:03<00:00,  4.84it/s, FSL_Acc=0.33]\n",
      "Meta Val Acc : 29.3978, Meta Val std: 0.5925, Time: 124.0\n",
      "  0%|                                                   | 0/600 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "100%|███████████████████████████| 600/600 [02:02<00:00,  4.88it/s, FSL_Acc=0.36]\n",
      "Meta Test Acc: 34.4644, Meta Test std: 0.7446, Time: 122.9\n",
      "==> Saving...\n",
      "  0%|                                                   | 0/600 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "100%|███████████████████████████| 600/600 [02:01<00:00,  4.93it/s, FSL_Acc=0.35]\n",
      "  0%|                                                   | 0/600 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "100%|███████████████████████████| 600/600 [02:00<00:00,  4.98it/s, FSL_Acc=0.35]\n",
      "Meta Val Acc : 29.5400, Meta Val std: 0.5924\n",
      "Meta Test Acc: 34.4689, Meta Test std: 0.7399\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 8184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           Meta Test std5 0.7246201354362984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               Train Loss 4.90734347597758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          Train Acc top 5 29.27083396911621\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                 Val Loss 3.242403362592061\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               _timestamp 1660040473.6650996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                  Val Acc 18.233333587646484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                Train Acc 9.413541793823242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            Meta Val std5 0.6086128987060421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             Meta Val std 0.5924891108131642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            Meta Val Acc5 39.88666666666666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           Meta Test Acc5 44.62\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             Meta Val Acc 29.397777777777772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            Meta Test std 0.74459140086806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                 _runtime 622.7259120941162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    _step 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            Val Acc top 5 47.32222366333008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            Meta Test Acc 34.46444444444444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    epoch 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    Final Meta Val Acc @1 29.54\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   Final Meta Test std @5 0.7263041638203673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   Final Meta Test Acc @5 44.62222222222222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   Final Meta Test Acc @1 34.46888888888889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    Final Meta Val Acc @5 39.82666666666667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    Final Meta Val std @5 0.6135955809344492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   Final Meta Test std @1 0.7399210071376701\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    Final Meta Val std @1 0.5923761811828666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing files in wandb/run-20220809_101051-11gsx335:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   dataloader.py\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval_fewshot.py\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   model.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_distillation.py\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_selfsupervison.py\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   util.py\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: plus 7 W&B file(s) and 1 media file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced stellar-morning-24: https://app.wandb.ai/geolep/FC100/runs/11gsx335\n"
     ]
    }
   ],
   "source": [
    "#Resnet_12_ssl\n",
    "!python /kaggle/working/SKD/train_selfsupervison.py --model resnet12_ssl --model_path save/FC100 --dataset FC100 --data_root /kaggle/input/few-shotcifar-100 --epochs 20 --lr_decay_epochs 8 --use_trainval True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
